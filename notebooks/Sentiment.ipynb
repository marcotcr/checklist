{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import Mft\n",
    "from checklist.inv_dir import Inv, Dir\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = checklist.editor.Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pp(data):\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    pr = wrapped_pp(data)[1][:, 1]\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent, amazing, experimental, incredible, emergency, American, good, great, international, elite, bad, extraordinary, ordinary, terrible, old, big, nice, wonderful, unusual, independent, enormous, interesting, different, beautiful, army, expensive, odd, experienced, important, new, older, active, invisible, fantastic, impressive, anonymous, empty, entire, Italian, special\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('It was {a:bert} {air_noun}.')[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'tough', 'weird', 'aggressive', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoyed, liked, like, appreciate, enjoy, appreciated, loved, love, miss, missed, recommend, needed, wanted, need, got, likes, hate, prefer, value, admired, enjoying, want, enjoys, admire, dislike, respected, respect, liking, did, dig, underestimated, use, trust, used, valued, helped, get, adore, understand, found, loves, have, feel, ,, cherish, was, LOVE, noticed, do, praised, tried, preferred, about, regret, supported, disliked, mean, compliment, had, bought, support, rate, took, left, is, dug, applaud, treasure, thanks, beat, thank, help, commend, improved, know, underestimate, remember, wish, welcome, hated, owe, saw, see, hit, embrace, to, for, take, lost, felt, impressed, all, think, are, recommended, leave, believe, thought, blame, understood, changed, envy, trusted, respects, started, embraced, met, credit, misses, follow, made, thanked, consider, Love, salute, told, meet, pleased, finished, packed, surprised, forgive, recruited, hired, tested, recognize, reviewed, into, picked, try, deserved, hope, chose, congratulate, praise, complement, regretted, loving, experienced, sold, drove, needs, welcomed, rocked, rode, fancy, spoiled, values, flew, survived, received, in, upgraded, delivered, remembered, worked, find, improve, crashed, earned, cherished, saved, ordered, offer, expanded, deliver, utilize, brought, provide, watched, hire, doubt, own, on, joined, uses, experience, disappointed, pushed, favor, honor, using, developed, worth, review, considered, expand, compliments, enhanced, implemented, returned, evolved, offered, extend, planned, managed, ended, regrets, spent, followed\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('I really {bert} the {air_noun}.')[:200]))\n",
    "# print()\n",
    "# print(', '.join(editor.suggest('I {bert} the {air_noun}.')[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 34 examples\n",
      "Test cases:      34\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test = Mft(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 37 examples\n",
      "Test cases:      37\n",
      "Fails (rate):    2 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) tough\n",
      "2 (1.0) aggressive\n"
     ]
    }
   ],
   "source": [
    "test = Mft(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 13 examples\n",
      "Test cases:      13\n",
      "Fails (rate):    13 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) see\n",
      "2 (1.0) Australian\n",
      "2 (1.0) Italian\n"
     ]
    }
   ],
   "source": [
    "test = Mft(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8970 examples\n",
      "Test cases:      8970\n",
      "Fails (rate):    173 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) It was an aggressive cabin crew.\n",
      "2 (1.0) That company is aggressive.\n",
      "2 (1.0) That service was aggressive.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels = [2] * len(data)\n",
    "data += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1716 examples\n",
      "Test cases:      1716\n",
      "Fails (rate):    1632 (95.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.7) The staff is private.\n",
      "0 (0.7) This was a British cabin crew.\n",
      "2 (1.0) That was an Israeli customer service.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'])\n",
    "data += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])\n",
    "data += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensifiers and reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolutely , really , very , extremely , incredibly , pretty , truly , amazingly , ... , quite , exceptionally , unbelievably , especially , equally , extraordinarily , almost , actually , most , absolute , obviously , overall , just , insanely , otherwise , already , utterly , unexpectedly , all , entirely , undeniably , ‚Ä¶ , exceedingly , unusually , even , always , enormously , amazing , immensely , increasingly , overwhelmingly , also , incredible , simply , altogether , actual , inherently , extra , historically , honestly , an\n"
     ]
    }
   ],
   "source": [
    "print(' , '.join(editor.suggest('{it} {be} {a:bert} {pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'])[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_adj = ['very', 'really', 'absolutely', 'truly', 'extremely', 'quite', 'incredibly', 'amazingly', 'especially', 'exceptionally', 'unbelievably', 'utterly', 'exceedingly', 'rather', 'totally', 'particularly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I, really, always, just, all, absolutely, truly, certainly, also, definitely, we, still, personally, actually, so, greatly, especially, thoroughly, both, sure, much, particularly, obviously, 'd, totally, people, genuinely, very, simply, you, sincerely, clearly, fully, highly, guys, quite, ,, We, deeply, honestly, completely, have, do, and, most, they, strongly, would, too, seriously, generally, 've, did, ly, dearly, never, will, 's, already, 'll, REALLY, immediately, everyone, should, only, even, family, again, must, kids, friends, had, students, rather, now, can, ..., does, ever, who, that, he, to, surely, many, probably, feel, parents, extremely, initely, customers, specifically, gladly, fucking, o, ers, everybody, enthusiastically, hugely, usually\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(editor.suggest('{i} {bert} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intens_verb = [ 'really', 'absolutely', 'truly', 'extremely',  'especially',  'utterly',  'totally', 'particularly', 'highly', 'definitely', 'certainly', 'genuinely', 'honestly', 'strongly', 'sure', 'sincerely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)\n",
    "non_neutral_pred = lambda pred, *args, **kwargs: pred != 1\n",
    "monotonic_label = Expect.slice_pairwise(monotonic_label, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:      2000\n",
      "After filtering: 1998 (99.9%)\n",
      "Fails (rate):    23 (1.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) It was a difficult aircraft.\n",
      "2 (1.0) It was an amazingly difficult aircraft.\n",
      "\n",
      "0 (0.7) This was an average aircraft.\n",
      "1 (0.7) This was an absolutely average aircraft.\n",
      "\n",
      "0 (0.9) It was a creepy food.\n",
      "2 (1.0) It was an exceedingly creepy food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:pos_adj} {air_noun}.', '{it} {be} {a:intens} {pos_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {pos_verb} {the} {air_noun}.', '{i} {intens} {pos_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "data += editor.template(['{it} {be} {a:neg_adj} {air_noun}.', '{it} {be} {a:intens} {neg_adj} {air_noun}.'] , intens=intens_adj, it=['It', 'This', 'That'], be=['is', 'was'], nsamples=500)\n",
    "data += editor.template(['{i} {neg_verb} {the} {air_noun}.', '{i} {intens} {neg_verb} {the} {air_noun}.'], intens=intens_verb, i=['I', 'We'], the=['this', 'that', 'the'], nsamples=500)\n",
    "test = Dir(data, monotonic_label)\n",
    "test.run(new_pp)\n",
    "test.set_monotonic_print(increasing=True)\n",
    "test.summary(3)\n",
    "# test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_adj = ['somewhat', 'kinda', 'mostly', 'probably', 'generally', 'reasonably', 'a little', 'a bit', 'slightly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)\n",
    "monotonic_label_down = Expect.slice_pairwise(monotonic_label_down, non_neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:      2000\n",
      "After filtering: 5 (0.2%)\n",
      "Fails (rate):    3 (60.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) This crew was tough.\n",
      "0 (1.0) This crew was kinda tough.\n",
      "\n",
      "0 (0.9) That crew is tough.\n",
      "0 (1.0) That crew is a little tough.\n",
      "\n",
      "0 (0.8) This crew was tough.\n",
      "0 (1.0) This crew was a bit tough.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {air_noun} {be} {pos_adj}.', '{it} {air_noun} {be} {red} {pos_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "data += editor.template(['{it} {air_noun} {be} {neg_adj}.', '{it} {air_noun} {be} {red} {neg_adj}.'] , red=reducer_adj, it=['The', 'This', 'That'], be=['is', 'was'], nsamples=1000)\n",
    "test = Dir(data, monotonic_label_down)\n",
    "test.run(new_pp)\n",
    "test.set_monotonic_print(increasing=False)\n",
    "test.summary(3)\n",
    "# test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "# test.run(new_pp)\n",
    "# test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariance: change neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = set(\n",
    "    ['.', 'the', 'The', ',', 'a', 'A', 'and', 'of', 'to', 'it', 'that', 'in',\n",
    "     'this', 'for',  'you', 'there', 'or', 'an', 'by', 'about', 'flight', 'my',\n",
    "     'in', 'of', 'have', 'with', 'was', 'at', 'it', 'get', 'from', 'this', 'Flight', 'plane'\n",
    "    ])\n",
    "forbidden = set(['No', 'no', 'Not', 'not', 'Nothing', 'nothing', 'without'] + pos_adj + neg_adj + pos_verb_present + pos_verb_past + neg_verb_present + neg_verb_past)\n",
    "def change_neutral(d):\n",
    "#     return d.text\n",
    "    examples = []\n",
    "    words_in = [x for x in d.capitalize().split() if x in neutral_words]\n",
    "    if not words_in:\n",
    "        return None\n",
    "    for w in words_in:\n",
    "        examples.extend([x[1] for x in editor.suggest_replace(d, w, beam_size=5, words_and_sentences=True) if x[0] not in forbidden])\n",
    "    if examples:\n",
    "        idxs = np.random.choice(len(examples), min(len(examples), 10), replace=False)\n",
    "        return [examples[i] for i in idxs]\n",
    "# Perturb.perturb(parsed_data[:5], perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Perturb.perturb(sentences, change_neutral, nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4989 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    55 (11.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @united Greetings. UA Club member here. Any idea if I can use the Air Canada Lounge at YVR. Flying UA tomorrow.\n",
      "1 (0.5) @united Greetings. UA Club member here. Any idea if I can use your Air Canada Lounge at YVR. Flying UA tomorrow.\n",
      "\n",
      "0 (0.8) @USAirways it still says that I can't check into my flight because the information is incorrect but everything is entered correctly\n",
      "1 (0.7) @USAirways it still says that I can't check into my portal because the information is incorrect but everything is entered correctly\n",
      "1 (0.8) @USAirways it still says that I can't check into this flight because the information is incorrect but everything is entered correctly\n",
      "\n",
      "1 (0.9) @americanair the best is your 800 message saying to use website and your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "2 (0.9) @americanair the best is your 800 message saying to use website . your website is saying you need to call.  If you don't answer, #hardtodo\n",
      "0 (0.7) @americanair the best is your 800 message saying to use website and your website is saying we need to call.  If we don't answer, #hardtodo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = editor.template('I {pos_verb_present} you.')\n",
    "positive += editor.template('You are {pos_adj}.')\n",
    "positive += ['I would fly with you again.']\n",
    "positive.remove('You are happy.')\n",
    "negative = editor.template('I {neg_verb_present} you.')\n",
    "negative += editor.template('You are {neg_adj}.')\n",
    "negative += ['Never flying with you again.']\n",
    "def add_phrase_function(phrases):\n",
    "    def pert(d):\n",
    "        while d[-1].pos_ == 'PUNCT':\n",
    "            d = d[:-1]\n",
    "        d = d.text\n",
    "        ret = [d + '. ' + x for x in phrases]\n",
    "        idx = np.random.choice(len(ret), 10, replace=False)\n",
    "        ret = [ret[i] for i in idx]\n",
    "        return ret\n",
    "    return pert\n",
    "\n",
    "# perturbed = PerturbFactory.perturb_key(small, 'sentence', add_phrase_function(positive))\n",
    "# test = mltest.Test(perturbed, expectation_fn = mon_increasing)\n",
    "# r = test.run(model.predict_and_confidence, is_binary=False, n=500)\n",
    "# r.summary(5, format_fn=format_perturb_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_1 = Expect.monotonic(label=2, increasing=True, tolerance=0.1)\n",
    "monotonic_1_down = Expect.monotonic(label=2, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5500 examples\n",
      "Test cases:      500\n",
      "After filtering: 148 (29.6%)\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, add_phrase_function(positive), nsamples=500)\n",
    "test = Dir(data, monotonic_1)\n",
    "test.run(new_pp, overwrite=True)\n",
    "test.set_monotonic_print(label=2, increasing=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5500 examples\n",
      "Test cases:      500\n",
      "After filtering: 389 (77.8%)\n",
      "Fails (rate):    84 (21.6%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) @JetBlue @KyleJudah It doesn't matter who you directed me to. It's the principle of the matter. When I gate checked the stroller it was\n",
      "2 (0.8) @JetBlue @KyleJudah It doesn't matter who you directed me to. It's the principle of the matter. When I gate checked the stroller it was. You are tough.\n",
      "0 (0.7) @JetBlue @KyleJudah It doesn't matter who you directed me to. It's the principle of the matter. When I gate checked the stroller it was. You are aggressive.\n",
      "\n",
      "0 (1.0) @SouthwestAir Logically you would think you check all that before you have people board. I could've drove home in the time I've been waiting\n",
      "0 (0.7) @SouthwestAir Logically you would think you check all that before you have people board. I could've drove home in the time I've been waiting. You are tough.\n",
      "\n",
      "0 (1.0) .@USAirways It was this saturday during all the snow in the DC area.  Excited the flight was not Cancelled Flightled but sad he went missing.\n",
      "0 (0.8) .@USAirways It was this saturday during all the snow in the DC area.  Excited the flight was not Cancelled Flightled but sad he went missing. You are aggressive.\n",
      "0 (0.9) .@USAirways It was this saturday during all the snow in the DC area.  Excited the flight was not Cancelled Flightled but sad he went missing. You are tough.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, add_phrase_function(negative), nsamples=500)\n",
    "test = Dir(data, monotonic_1_down)\n",
    "test.run(new_pp, overwrite=True)\n",
    "test.set_monotonic_print(label=2, increasing=False)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: robustness\n",
    "### Invariance: adding irrelevant stuff before and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "# data['sentence']\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    53 (10.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) @JetBlue can your people working this contact me, I have a project in the works.\n",
      "1 (0.6) @JetBlue can your people working this contact me, I have a project in the works. https://t.co/tjLXUd\n",
      "1 (0.6) @JetBlue can your people working this contact me, I have a project in the works. https://t.co/TyxgCd\n",
      "\n",
      "2 (1.0) @united Step 1: Cancelled Flight flight. Step 2: Don't notify customer. Step 3: Charge them for food while they try to survive their wait. Brilliant.\n",
      "1 (0.6) @united Step 1: Cancelled Flight flight. Step 2: Don't notify customer. Step 3: Charge them for food while they try to survive their wait. Brilliant. https://t.co/VXNOpK\n",
      "0 (0.7) @united Step 1: Cancelled Flight flight. Step 2: Don't notify customer. Step 3: Charge them for food while they try to survive their wait. Brilliant. @Q67bDi\n",
      "\n",
      "2 (0.9) @USAirways @AmericanAir any help regarding flights out of KPHL would be much appreciated\n",
      "1 (0.6) @USAirways @AmericanAir any help regarding flights out of KPHL would be much appreciated @y3YuRw\n",
      "1 (0.6) https://t.co/EBsb4S @USAirways @AmericanAir any help regarding flights out of KPHL would be much appreciated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, add_irrelevant, nsamples=500)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation, contractions, typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1176 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    31 (6.2%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) @JetBlue is that one on the picture http://t.co/lxwbsfxfj0\n",
      "2 (0.9) @JetBlue is that one on the picture\n",
      "2 (1.0) @JetBlue is that one on the picture.\n",
      "\n",
      "2 (0.8) @JetBlue with the free wifi #impressive #FlyFi http://t.co/T1RYpzEBc8\n",
      "1 (0.7) @JetBlue with the free wifi #impressive #FlyFi http://t.co/T1RYpzEBc8.\n",
      "\n",
      "0 (0.8) @united they had record of it being at Denver on the concourse prior to me gettin on the shuttle. I just want to confirm its location\n",
      "1 (0.7) @united they had record of it being at Denver on the concourse prior to me gettin on the shuttle. I just want to confirm its location.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=500)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    37 (7.4%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.7) @USAirways thanks for the seat that doesn't recline. I'm shocked I'm not being asked to serve everyone drinks on the plane.  #DoBetter\n",
      "1 (0.9) @USAirways thanks for the seat that doesn't recline. I'm shocked I'm not being asked to serve veeryone drinks on the plane.  #DoBetter\n",
      "\n",
      "2 (0.7) @AmericanAir on Feb. 15th your rep gave me the record locator and told me I'd be receiving an email with the itinerary and confirmation.\n",
      "0 (0.7) @AmericanAir on Feb. 15t hyour rep gave me the record locator and told me I'd be receiving an email with the itinerary and confirmation.\n",
      "\n",
      "0 (1.0) @AmericanAir this has to be the absolute WORST EXPERIENCE EVER!\n",
      "2 (1.0) @AmericanAir this has to be the absolute OWRST EXPERIENCE EVER!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=1)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      500\n",
      "Fails (rate):    44 (8.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) .@USAirways I did but the more eyes I have looking for Pandu the better chance I have of bringing him home.\n",
      "0 (1.0) .@USAirways I did but the more eyes I have looking for Pandu hte better cahnce I have of bringing him home.\n",
      "\n",
      "1 (0.7) @USAirways - done :)\n",
      "0 (1.0) @USAirwasy -d one :)\n",
      "\n",
      "2 (1.0) @united Just sent! Thanks :)\n",
      "0 (1.0) @united Just snet! hTanks :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=500, typos=2)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2076 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    26 (2.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.9) @JetBlue spoken to 2 reps. Once I'm allowed to check my bag and through the TSA checkpoint, I guarantee I will be talking to someone.\n",
      "1 (1.0) @JetBlue spoken to 2 reps. Once I'm allowed to check my bag and through the TSA checkpoint, I guarantee I'll be talking to someone.\n",
      "\n",
      "0 (0.8) @united I would appreciate a response regarding the pressurization failure on flight 1109. You seem to be responding to less serious issues\n",
      "1 (0.8) @united I'd appreciate a response regarding the pressurization failure on flight 1109. You seem to be responding to less serious issues\n",
      "\n",
      "1 (0.6) @AmericanAir what's the best number to use?\n",
      "0 (1.0) @AmericanAir what is the best number to use?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(sentences, Perturb.contractions, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 3641 examples\n",
      "Test cases:      331\n",
      "Fails (rate):    18 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.8) @JetBlue okay. Names of SJ crew are Alamo, Tatiana. Keep telling me to hold, then leave desk. Ali told me he couldn't help bc going on break\n",
      "1 (0.8) @JetBlue okay. Names of SJ crew are Alamo, Tatiana. Keep telling me to hold, then leave desk. Christopher told me he couldn't help bc going on break\n",
      "1 (0.7) @JetBlue okay. Names of SJ crew are Alamo, Tatiana. Keep telling me to hold, then leave desk. Logan told me he couldn't help bc going on break\n",
      "\n",
      "1 (0.6) ‚Äú@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Andrew. What can we do for you?‚Äù SMA\n",
      "0 (0.7) ‚Äú@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Justin. What can we do for you?‚Äù SMA\n",
      "0 (0.7) ‚Äú@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Benjamin. What can we do for you?‚Äù SMA\n",
      "\n",
      "1 (1.0) @VirginAmerica Plans to Include Austin to its Dallas Route - TopNews Arab #Emirates http://t.co/aqZWecOkk2\n",
      "2 (0.7) @VirginAmerica Plans to Include Connor to its Dallas Route - TopNews Arab #Emirates http://t.co/aqZWecOkk2\n",
      "2 (0.8) @VirginAmerica Plans to Include Zachary to its Dallas Route - TopNews Arab #Emirates http://t.co/aqZWecOkk2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9999 examples\n",
      "Test cases:      909\n",
      "Fails (rate):    62 (6.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) @USAirways I didn't even leave the airport and you sent 2 of my bags to Philadelphia!\n",
      "0 (0.7) @USAirways I didn't even leave the airport and you sent 2 of my bags to St. Paul!\n",
      "1 (0.7) @USAirways I didn't even leave the airport and you sent 2 of my bags to Diamond Bar!\n",
      "\n",
      "2 (0.9) @SouthwestAir my friends from Boston stuck in Denver. Her name Jane. @RnCahill  Please contact her.\n",
      "1 (0.8) @SouthwestAir my friends from Boston stuck in Fresno. Her name Jane. @RnCahill  Please contact her.\n",
      "\n",
      "0 (0.7) @USAirways your delayed flight out of Wilmington made me miss my flight out of Charlotte. Figure out how to take off and arrive on time.\n",
      "1 (0.6) @USAirways your delayed flight out of Wilmington made me miss my flight out of Monterey Park. Figure out how to take off and arrive on time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 11000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    19 (1.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) @VirginAmerica Can I get some help with a support ticket? It's been 15 days.... Incident: 150202-000419 Thank you!\n",
      "1 (0.8) @VirginAmerica Can I get some help with a support ticket? It's been 15 days.... Incident: 150202-442 Thank you!\n",
      "\n",
      "2 (0.8) @united MIA-EWR #384 üòÑüòÑüòÑ excellent crew. EWR-IAD #3589 üò°üò°üò° No crew to load bags - waiting w/ door open freezing. 20 mins past departure.\n",
      "1 (0.7) @united MIA-EWR #318 üòÑüòÑüòÑ excellent crew. EWR-IAD #3589 üò°üò°üò° No crew to load bags - waiting w/ door open freezing. 20 mins past departure.\n",
      "\n",
      "1 (0.8) @USAirways it doesn't take 6 days to respond to an already open case!\n",
      "0 (0.7) @USAirways it doesn't take 5 days to respond to an already open case!\n",
      "0 (0.9) @USAirways it doesn't take 7 days to respond to an already open case!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(parsed_data, Perturb.change_number, nsamples=1000)\n",
    "test = Inv(data)\n",
    "test.run(new_pp)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect: temporal awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'dislike', 'regret', 'abhor', 'dread', 'despise']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{neg_verb_present}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8000 examples\n",
      "Test cases:      8000\n",
      "Fails (rate):    1532 (19.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I think this airline is excellent, but I used to think it was ridiculous.\n",
      "2 (1.0) I dislike this airline, but I used to enjoy it.\n",
      "2 (1.0) I think this airline is boring, but in the past I thought it was wonderful.\n"
     ]
    }
   ],
   "source": [
    "change = ['but', 'even though', 'although', '']\n",
    "data = editor.template(['I used to think this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} I used to think it was {neg_adj}.',\n",
    "                                 'In the past I thought this airline was {neg_adj}, {change} now I think it is {pos_adj}.',\n",
    "                                 'I think this airline is {pos_adj}, {change} in the past I thought it was {neg_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500)\n",
    "data += editor.template(['I used to {neg_verb_present} this airline, {change} now I {pos_verb_present} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} I used to {neg_verb_present} it.',\n",
    "                                 'In the past I would {neg_verb_present} this airline, {change} now I {pos_verb} it.',\n",
    "                                 'I {pos_verb_present} this airline, {change} in the past I would {neg_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500)\n",
    "labels = [2] * len(data)\n",
    "\n",
    "data += editor.template(['I used to think this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} I used to think it was {pos_adj}.',\n",
    "                                 'In the past I thought this airline was {pos_adj}, {change} now I think it is {neg_adj}.',\n",
    "                                 'I think this airline is {neg_adj}, {change} in the past I thought it was {pos_adj}.',\n",
    "                                ] ,\n",
    "                                 change=change, unroll=True, nsamples=500)\n",
    "data += editor.template(['I used to {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} I used to {pos_verb_present} it.',\n",
    "                                 'In the past I would {pos_verb_present} this airline, {change} now I {neg_verb_present} it.',\n",
    "                                 'I {neg_verb_present} this airline, {change} in the past I would {pos_verb_present} it.',\n",
    "                                ] ,\n",
    "                                change=change, unroll=True, nsamples=500)\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = Mft(data, labels=labels)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to should reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9048 examples\n",
      "Test cases:      4524\n",
      "After filtering: 15 (0.3%)\n",
      "Fails (rate):    8 (53.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.7) this was an average aircraft.\n",
      "0 (1.0) I used to think this was an average aircraft.\n",
      "\n",
      "0 (0.9) it was an average flight.\n",
      "0 (1.0) I used to think it was an average flight.\n",
      "\n",
      "0 (0.8) this is an average flight.\n",
      "0 (1.0) I used to think this is an average flight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(['{it} {be} {a:adj} {air_noun}.', 'I used to think {it} {be} {a:adj} {air_noun}.'], it=['it', 'this', 'that'], be=['is', 'was'], adj=editor.lexicons['pos_adj'] + editor.lexicons['neg_adj'])\n",
    "data += editor.template(['{i} {verb} {the} {air_noun}.', '{i} used to {verb} {the} {air_noun}.'], i=['I', 'We'], the=['this', 'that', 'the'], verb=editor.lexicons['pos_verb_present'] + editor.lexicons['neg_verb_present'])\n",
    "test = Dir(data, monotonic_label_down)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect:fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', 'female', 'first_name', 'first_pronoun', 'last_name', 'country', 'nationality', 'city', 'religion', 'religion_adj', 'sexual_adj', 'air_noun', 'pos_adj', 'neg_adj', 'neutral_adj', 'pos_verb_present', 'neg_verb_present', 'neutral_verb_present', 'pos_verb_past', 'neg_verb_past', 'neutral_verb_past', 'pos_verb', 'neg_verb', 'neutral_verb'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a Christian',\n",
       "  'a Protestant',\n",
       "  'a Roman Catholic',\n",
       "  'an Eastern Orthodox',\n",
       "  'an Anglican',\n",
       "  'a Jew',\n",
       "  'an Orthodox Jew',\n",
       "  'a Muslim',\n",
       "  'a Sunni',\n",
       "  \"a Shi'a\",\n",
       "  'an Ahmadiyya',\n",
       "  'a Buddhist',\n",
       "  'a Zoroastrian',\n",
       "  'a Hindu',\n",
       "  'a Sikh',\n",
       "  'a Shinto',\n",
       "  \"a Baha'i\",\n",
       "  'a Taoist',\n",
       "  'a Confucian',\n",
       "  'a Jain',\n",
       "  'an Atheist',\n",
       "  'an Agnostic'],)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{a:religion_adj}'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = {\n",
    "    'race': ['a black','a hispanic', 'a white', 'an asian'],\n",
    "    'sexual': editor.template('{a:sexual_adj}'),\n",
    "    'religion': editor.template('{a:religion_adj}'),\n",
    "    'nationality': editor.template('{a:nationality}')[:20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "Predicting 800 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    190 (95.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) Heather is a black Muslim.\n",
      "2 (0.8) Heather is an asian Muslim.\n",
      "1 (1.0) Heather is a hispanic Muslim.\n",
      "\n",
      "0 (0.9) Tara is a black American.\n",
      "2 (1.0) Tara is an asian American.\n",
      "2 (1.0) Tara is a hispanic American.\n",
      "\n",
      "0 (0.9) James is a black Muslim.\n",
      "2 (0.8) James is a hispanic Muslim.\n",
      "2 (0.9) James is an asian Muslim.\n",
      "\n",
      "\n",
      "0.21 a black\n",
      "1.94 a hispanic\n",
      "0.39 a white\n",
      "2.00 an asian\n",
      "\n",
      "\n",
      "-------------------------\n",
      "sexual\n",
      "Predicting 2800 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    200 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Bryan is an asexual man.\n",
      "0 (0.8) Bryan is a queer man.\n",
      "0 (0.9) Bryan is a lesbian man.\n",
      "\n",
      "2 (1.0) Nicole is an asexual child.\n",
      "0 (0.8) Nicole is a heterosexual child.\n",
      "1 (0.8) Nicole is a queer child.\n",
      "\n",
      "2 (1.0) Kelsey is an asexual person.\n",
      "0 (0.7) Kelsey is a transsexual person.\n",
      "0 (0.7) Kelsey is a bisexual person.\n",
      "\n",
      "\n",
      "1.74 an asexual\n",
      "1.56 a bisexual\n",
      "0.65 a heterosexual\n",
      "0.01 a homosexual\n",
      "1.74 a pansexual\n",
      "1.30 a queer\n",
      "1.56 a transsexual\n",
      "0.38 a trans\n",
      "0.02 a gay\n",
      "1.74 a straight\n",
      "0.21 a transgender\n",
      "0.28 a lesbian\n",
      "1.92 a non-binary\n",
      "1.70 a cisgender\n",
      "\n",
      "\n",
      "-------------------------\n",
      "religion\n",
      "Predicting 4400 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    195 (97.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) Shawn is a Christian writer.\n",
      "0 (1.0) Shawn is an Agnostic writer.\n",
      "0 (1.0) Shawn is an Atheist writer.\n",
      "\n",
      "2 (1.0) Justin is a Christian writer.\n",
      "1 (1.0) Justin is a Muslim writer.\n",
      "0 (1.0) Justin is an Agnostic writer.\n",
      "\n",
      "2 (1.0) Rebecca is a Christian Jew.\n",
      "1 (0.7) Rebecca is a Buddhist Jew.\n",
      "0 (0.9) Rebecca is an Agnostic Jew.\n",
      "\n",
      "\n",
      "1.99 a Christian\n",
      "1.97 a Protestant\n",
      "1.92 a Roman Catholic\n",
      "2.00 an Eastern Orthodox\n",
      "1.99 an Anglican\n",
      "1.42 a Jew\n",
      "1.98 an Orthodox Jew\n",
      "1.61 a Muslim\n",
      "1.95 a Sunni\n",
      "1.98 a Shi'a\n",
      "1.98 an Ahmadiyya\n",
      "1.83 a Buddhist\n",
      "2.00 a Zoroastrian\n",
      "1.83 a Hindu\n",
      "1.99 a Sikh\n",
      "1.94 a Shinto\n",
      "2.00 a Baha'i\n",
      "2.00 a Taoist\n",
      "1.84 a Confucian\n",
      "2.00 a Jain\n",
      "0.10 an Atheist\n",
      "0.39 an Agnostic\n",
      "\n",
      "\n",
      "-------------------------\n",
      "nationality\n",
      "Predicting 4000 examples\n",
      "Test cases:      200\n",
      "Fails (rate):    10 (5.0%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) Caleb is a Chinese Muslim.\n",
      "1 (0.5) Caleb is a Russian Muslim.\n",
      "1 (0.6) Caleb is a Pakistani Muslim.\n",
      "\n",
      "2 (0.8) Nathan is a Chinese Muslim.\n",
      "1 (0.6) Nathan is a German Muslim.\n",
      "\n",
      "2 (0.9) Christian is a Chinese Muslim.\n",
      "0 (0.7) Christian is a German Muslim.\n",
      "\n",
      "\n",
      "2.00 a Chinese\n",
      "2.00 an Indian\n",
      "2.00 an American\n",
      "2.00 an Indonesian\n",
      "1.98 a Pakistani\n",
      "2.00 a Brazilian\n",
      "2.00 a Nigerian\n",
      "2.00 a Bangladeshi\n",
      "1.99 a Russian\n",
      "2.00 a Japanese\n",
      "2.00 a Mexican\n",
      "2.00 an Ethiopian\n",
      "2.00 a Philippine\n",
      "2.00 an Egyptian\n",
      "2.00 a Vietnamese\n",
      "1.92 a German\n",
      "1.99 a Turkish\n",
      "2.00 an Iranian\n",
      "2.00 a Thai\n",
      "2.00 a French\n",
      "\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for p, vals in protected.items():\n",
    "    print(p)\n",
    "    d = editor.template(['{male} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=100)\n",
    "    d += editor.template(['{female} is %s {bert}.' % r for r in vals], return_maps=False, nsamples=100)\n",
    "    test = Inv(d, threshold=0.1)\n",
    "    test.run(new_pp)\n",
    "    test.summary(n=3)\n",
    "    print()\n",
    "    preds = np.array(test.results.preds)\n",
    "    for i, x in enumerate(vals):\n",
    "        print('%.2f %s' % (preds[:, i].mean(), vals[i]))\n",
    "    print()\n",
    "    print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 6318 examples\n",
      "Test cases:      6318\n",
      "Fails (rate):    580 (9.2%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I would never say I love that airline.\n",
      "2 (1.0) I can't say I appreciate that customer service.\n",
      "2 (1.0) I would never say I enjoy this crew.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "data += editor.template('No one {pos_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7254 examples\n",
      "Test cases:      7254\n",
      "Fails (rate):    1255 (17.3%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) This isn't an aggressive pilot.\n",
      "0 (1.0) No one dislikes that pilot.\n",
      "0 (1.0) I would never say I despise this company.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "data += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "is_not_0 = lambda x, pred, *args: pred != 0\n",
    "test = Mft(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2496 examples\n",
      "Test cases:      2496\n",
      "Fails (rate):    2466 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) It isn't an Indian service.\n",
      "0 (1.0) That was not an American service.\n",
      "0 (1.0) That was not an international flight.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=['This', 'That', 'The'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], benot=['is not',  'isn\\'t', 'was not', 'wasn\\'t'])\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "data += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2106 examples\n",
      "Test cases:      2106\n",
      "Fails (rate):    32 (1.5%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) I thought I would love this aircraft, but I did not.\n",
      "2 (0.8) I thought I would admire that service, but I did not.\n",
      "1 (0.8) I thought I would admire that seat, but I did not.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "test = Mft(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2418 examples\n",
      "Test cases:      2418\n",
      "Fails (rate):    2082 (86.1%)\n",
      "\n",
      "Example fails:\n",
      "0 (0.9) I thought that pilot would be sad, but it was not.\n",
      "0 (1.0) I thought that customer service would be tough, but it was not.\n",
      "0 (1.0) I thought this food would be poor, but it wasn't.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "test = Mft(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 858 examples\n",
      "Test cases:      858\n",
      "Fails (rate):    844 (98.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I thought the service would be Australian, but it wasn't.\n",
      "0 (1.0) I thought that flight would be commercial, but it wasn't.\n",
      "0 (1.0) I thought the seat would be commercial, but it was not.\n"
     ]
    }
   ],
   "source": [
    "data = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', neg=['was not', 'wasn\\'t'], it=['this', 'that', 'the'], nt=['is not', 'isn\\'t'])\n",
    "data += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=['this', 'that', 'the'])\n",
    "# expectation: prediction is not 0\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harder: negation with neutral in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    731 (73.1%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say, given all that I've seen over the years, that that plane is fun.\n",
      "2 (1.0) I don't think I, given the time that I've been flying, that we admire that company.\n",
      "2 (1.0) I can't say, given that I am from Brazil, that that staff is beautiful.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = Mft(data, labels=0)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    994 (99.4%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) I can't say, given the time that I've been flying, that that was an awful aircraft.\n",
      "0 (1.0) I can't say, given all that I've seen over the years, that this is a weird pilot.\n",
      "0 (1.0) I can't say, given the time that I've been flying, that that crew is horrible.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = Mft(data, Expect.single(is_not_0))\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    976 (97.6%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I can't say I, given my history with airplanes, that we see the service.\n",
      "0 (1.0) I wouldn't say, given all that I've seen over the years, that this customer service was Israeli.\n",
      "0 (1.0) I don't think, given my history with airplanes, that the is a private customer service.\n"
     ]
    }
   ],
   "source": [
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.',neutral=neutral,  neg=['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say'], it=['this', 'that', 'the'], be=['is', 'was'])\n",
    "data += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.',neutral=neutral,  neg=neg, i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = list(np.random.choice(data, 1000, replace=False))\n",
    "test = Mft(data, labels=1)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aspect: SRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my opinion is more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9136 examples\n",
      "Test cases:      9136\n",
      "Fails (rate):    3374 (36.9%)\n",
      "\n",
      "Example fails:\n",
      "2 (1.0) I think you are rough, but some people think you are adorable.\n",
      "0 (0.8) I think you are good, but I had heard you were nasty.\n",
      "0 (0.9) I think you are exceptional, but I had heard you were sad.\n"
     ]
    }
   ],
   "source": [
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.',\n",
    "             'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',\n",
    "             ]\n",
    "data = editor.template(templates, change=change, unroll=True)\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.',\n",
    "             'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "labels = [2] * len(data)\n",
    "\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.',\n",
    "             'I think you are {neg_adj},{change} I had heard you were {pos_adj}.',\n",
    "             ]\n",
    "data += editor.template(templates, change=change, unroll=True)\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.',\n",
    "             'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',\n",
    "            ]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "test = Mft(data, labels=labels)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q & a form: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7956 examples\n",
      "Test cases:      7956\n",
      "Fails (rate):    226 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "2 (0.8) Did we dislike this seat? Yes\n",
      "0 (0.9) Do I think this airline was nice? Yes\n",
      "2 (0.7) Did we dislike that seat? Yes\n"
     ]
    }
   ],
   "source": [
    "temp = editor.template('Do I think {it} {air_noun} {be} {pos_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp += editor.template('Did {i} {pos_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = [x + ' Yes' for x in temp]\n",
    "labels = [2] * len(data)\n",
    "temp2 = editor.template('Do I think {it} {air_noun} {be} {neg_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp2 += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp2 += editor.template('Did {i} {neg_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data += [x + ' Yes' for x in temp2]\n",
    "labels += [0] * (len(data) - len(labels))\n",
    "\n",
    "test = Mft(data, labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1560 examples\n",
      "Test cases:      1560\n",
      "Fails (rate):    1541 (98.8%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think it is an Israeli crew? Yes\n",
      "0 (1.0) Do I think this was an Israeli aircraft? Yes\n",
      "0 (1.0) Do I think that food is Italian? Yes\n"
     ]
    }
   ],
   "source": [
    "temp3 = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}?', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "temp3 += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}?', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "temp3 += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}?', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "data = [x + ' Yes' for x in temp3]\n",
    "test = Mft(data, labels=1)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 7956 examples\n",
      "Test cases:      7956\n",
      "Fails (rate):    4371 (54.9%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that was a terrible crew? No\n",
      "0 (1.0) Do I think that is a bad customer service? No\n",
      "0 (1.0) Did we hate this pilot? No\n"
     ]
    }
   ],
   "source": [
    "data = [x + ' No' for x in temp]\n",
    "labels = [0] * len(data)\n",
    "data += [x + ' No' for x in temp2]\n",
    "labels += [1] * (len(data) - len(labels))\n",
    "\n",
    "allow_for_neutral = lambda x, pred, _, label, _2 : pred != 0 if label == 1 else pred == label\n",
    "\n",
    "test = Mft(data, Expect.single(allow_for_neutral), labels=labels)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1560 examples\n",
      "Test cases:      1560\n",
      "Fails (rate):    1560 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0 (1.0) Do I think that staff was British? No\n",
      "0 (1.0) Do I think the customer service was Italian? No\n",
      "0 (1.0) Do I think that is a private customer service? No\n"
     ]
    }
   ],
   "source": [
    "data = [x + ' No' for x in temp3]\n",
    "test = Mft(data, labels=1)\n",
    "# test = Mft(data, labels=2)\n",
    "test.run(new_pp)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

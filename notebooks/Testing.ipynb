{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "import spacy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import Mft\n",
    "from checklist.inv_dir import Inv, Dir\n",
    "from checklist.expect import Expect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = checklist.text_generation.TextGenerator(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg.unmask_multiple(['I really <mask> the pilot.', 'I really <mask> the flight.'], metric='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "editor = checklist.editor.Editor()\n",
    "c = pickle.load(open('/home/marcotcr/work/ml-tests/data/common.pkl', 'rb'))\n",
    "editor.lexicons['male'] = c['male_names']\n",
    "editor.lexicons['female'] = c['female_names']\n",
    "c.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = ['flight', 'seat', 'pilot', 'staff', 'plane', 'airline', 'cabin crew', 'aircraft', 'food']\n",
    "pos = ['liked', 'enjoyed', 'loved', 'admired', 'appreciated']\n",
    "neg = ['hated', 'disliked', 'regretted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'liked', 'love', 'enjoyed', 'loved', 'appreciate', 'enjoy', 'appreciated', 'miss', 'Like', 'missed', 'needed', 'hate', 'wanted', 'likes', 'got', 'admired', 'admire', 'hated', 'recommend']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('I really <mask> the {n}.', n=nouns)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000)][:20])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'We', 'He', 'he', 'She', 'They', 'we', 'and', '...', 'she', 'they', 'Everyone', 'i', 'People', 'You', 'Alex', 'Everybody', 'And', 'Dad', 'John']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('<mask> really {pn} the {n}.', n=nouns, pn=pos+neg)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000)][:20])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = ['I', 'We', 'They', 'we', 'He', 'he', 'She', 'she', 'they', 'people', 'People', 'you', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'boring', 'strange', 'cold', 'slow', 'unpleasant', 'weird', 'difficult', 'uncomfortable', 'expensive', 'rough', 'noisy', 'ugly', 'poor', 'dark', 'different', 'small', 'disappointing', 'quiet', 'rude', 'confusing', 'dull', 'sad', 'odd', 'loud', 'annoying', 'simple', 'harsh', 'short', 'unusual', 'scary', 'hard', 'nice', 'hot', 'dangerous', 'aggressive', 'heavy', 'dirty', 'conservative', 'old', 'tough', 'bland', 'low', 'cheap', 'frustrating', 'depressing', 'nasty', 'long', 'crude', 'horrible', 'stupid', 'complicated', 'funny', 'fast', 'good', 'basic', 'weak', 'wrong', 'awkward', 'terrible', 'negative', 'crowded', 'plain', 'thin', 'interesting', 'awful', 'bizarre', 'intimidating', 'cool', 'primitive', 'unstable', 'flat', 'narrow', 'stressful', 'stiff', 'empty', 'chaotic', 'light', 'irritating', 'big', 'unfair', 'sick', 'similar', 'angry', 'mean', 'predictable', 'easy', 'offensive', 'foreign', 'violent', 'tight', 'comfortable', 'busy', 'challenging', 'arrogant', 'disgusting', 'inefficient', 'creepy', 'tense', 'disturbing']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('I didn\\'t like the {n}, it was very <mask>.', n=nouns)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000, metric='avg')][:100])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'nice', 'helpful', 'comfortable', 'cool', 'efficient', 'pleasant', 'interesting', 'impressive', 'welcoming', 'professional', 'beautiful', 'exciting', 'positive', 'solid', 'amazing', 'wonderful', 'lovely']\n",
    "neg_adj = ['bad', 'boring', 'unpleasant', 'difficult', 'uncomfortable', 'ugly', 'poor', 'disappointing', 'sad', 'annoying', 'dirty', 'frustrating', 'depressing', 'nasty', 'horrible', 'stupid', 'negative', 'awful', 'stressful', 'irritating', 'disgusting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_eg = editor.template('{sub}{really} {p} the {n}.', sub=subj, really=[' really', ''], n=nouns, p=pos)\n",
    "pos_eg += editor.template('The {n} was{very} {p}.', sub=subj, very=[' very', ''], n=nouns, p=pos_adj)\n",
    "neg_eg = editor.template('{sub}{really} {p} the {n}.', sub=subj, really=[' really', ''], n=nouns, p=neg)\n",
    "neg_eg += editor.template('The {n} was{very} {p}.', sub=subj, very=[' very', ''], n=nouns, p=neg_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 1080)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_eg), len(neg_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = Mft(pos_eg, labels=1)\n",
    "test_neg = Mft(neg_eg, labels=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pos.set_expect(Expect.single(Expect.eq()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1494 examples\n",
      "Predicting 1080 examples\n"
     ]
    }
   ],
   "source": [
    "test_pos.run(pp)\n",
    "test_neg.run(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1080\n",
      "Failure rate:   1.7%\n",
      "\n",
      "Example fails:\n",
      "1 (1.0) The cabin crew was very stressful.\n",
      "1 (1.0) The flight was very stressful.\n",
      "1 (1.0) The flight was stressful.\n",
      "1 (1.0) The aircraft was stressful.\n",
      "1 (1.0) The seat was very stressful.\n"
     ]
    }
   ],
   "source": [
    "test_neg.summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1494\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "test_pos.summary(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inv, dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_typo(string):\n",
    "    string = list(string)\n",
    "    swaps = 1\n",
    "    swaps = np.random.choice(len(string) - 1, swaps)\n",
    "    for swap in swaps:\n",
    "        swap = np.random.choice(len(string) - 1)\n",
    "        tmp = string[swap]\n",
    "        string[swap] = string[swap + 1]\n",
    "        string[swap + 1] = tmp\n",
    "    return ''.join(string)\n",
    "\n",
    "def add_typos(string):\n",
    "    return list(set([add_typo(string) for _ in range(10)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 100), add_typos, keep_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1043 examples\n"
     ]
    }
   ],
   "source": [
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     100\n",
      "Failure rate:   24.0%\n",
      "\n",
      "Example fails:\n",
      "0.0 @JetBlue Not for the dates or destination I'm headed üòî\n",
      "0.5 @JetBlu eNot for the dates or destination I'm headed üòî\n",
      "\n",
      "0.5 @AmericanAir will do. I also passed the website around to other passengers.\n",
      "0.7 @AmericanAir will do. I also passed the website around to other psasengers.\n",
      "0.7 @AmericanAir will do. I also passed the ewbsite around to other passengers.\n",
      "\n",
      "1.0 @united DM'ed you\n",
      "0.1 @uinted DM'ed you\n",
      "0.1 @united DM'ed yuo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2000 examples\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_typo, keep_original=True)\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1000\n",
      "Failure rate:   4.9%\n",
      "\n",
      "Example fails:\n",
      "0.7 @united when will you offer real food in american clubs like the amazing food you offer in Heathrow?\n",
      "0.2 @uinted when will you offer real food in american clubs like the amazing food you offer in Heathrow?\n",
      "\n",
      "1.0 @united maybe on my return trip üëç\n",
      "0.1 @united maybe on my erturn trip üëç\n",
      "\n",
      "0.7 @united was in the air. Just DMd you\n",
      "0.0 @united was in the ai.r Just DMd you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get from somethwere else\n",
    "def add_negatives(string):\n",
    "    string = string.strip('.')\n",
    "    return [string + '. ' + l for l in ['I hate you', 'I despise you', 'You suck']]\n",
    "def add_positive(string):\n",
    "    string = string.strip('.')\n",
    "    return [string + '. ' + l for l in ['I love you', 'I like you', 'You are great']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:     1000\n",
      "Filtered cases: 239 (23.9%)\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_negatives, keep_original=True)\n",
    "expect_fn = Expect.pairwise(Expect.monotonic_label(1, increasing=False, tolerance=0.1))\n",
    "test = Dir(data, expect_fn)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=False)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:     1000\n",
      "Failure rate:   0.9%\n",
      "\n",
      "Example fails:\n",
      "1.0 @JetBlue we made it safe and sound. Thank you for the safe travels.\n",
      "1.0 @JetBlue we made it safe and sound. Thank you for the safe travels. I love you\n",
      "\n",
      "1.0 @JetBlue she was a phone agent, pls do! Peggy was pleasant, informative and delivered. ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n",
      "1.0 @JetBlue she was a phone agent, pls do! Peggy was pleasant, informative and delivered. ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è. I love you\n",
      "1.0 @JetBlue she was a phone agent, pls do! Peggy was pleasant, informative and delivered. ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è. I like you\n",
      "\n",
      "1.0 @JetBlue she was a phone agent, pls do! Peggy was pleasant, informative and delivered. ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n",
      "1.0 @JetBlue she was a phone agent, pls do! Peggy was pleasant, informative and delivered. ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è. I love you\n",
      "1.0 @JetBlue she was a phone agent, pls do! Peggy was pleasant, informative and delivered. ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è. I like you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_positive, keep_original=True)\n",
    "expect_fn = Expect.pairwise(Expect.monotonic_label(1, increasing=True, tolerance=0.0))\n",
    "test = Dir(data, expect_fn)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp = model_wrapper.ModelWrapper()\n",
    "pp = PredictorWrapper.wrap_softmax(qqp.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\tWhat does the Quran say about homosexuality?\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qs = []\n",
    "labels = []\n",
    "all_questions = set()\n",
    "for x in open('/home/marcotcr/datasets/glue/glue_data/QQP/dev.tsv').readlines()[1:]:\n",
    "    try:\n",
    "        q1, q2, label = x.strip().split('\\t')[3:]\n",
    "    except:\n",
    "        print(x)\n",
    "        continue\n",
    "    all_questions.add(q1)\n",
    "    all_questions.add(q2)\n",
    "    qs.append((q1, q2))\n",
    "    labels.append(label)\n",
    "labels = np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran = [x[0][0] for x in tg.unmask('What does the Quran say about <mask>?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran.remove('homosexuals')\n",
    "quran.remove('gays')\n",
    "quran.remove('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 272 examples\n",
      "Test cases:     272\n",
      "Failure rate:   2.2%\n",
      "\n",
      "Example fails:\n",
      "0.6 ('What does the Quran say about polygamy?', 'What does the Quran say about circumcision?')\n",
      "0.7 ('What does the Quran say about circumcision?', 'What does the Quran say about polygamy?')\n",
      "0.8 ('What does the Quran say about Muhammad?', 'What does the Quran say about Muslims?')\n",
      "1.0 ('What does the Quran say about Islam?', 'What does the Quran say about Muslims?')\n",
      "0.8 ('What does the Quran say about Muslims?', 'What does the Quran say about Islam?')\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(('What does the Quran say about {thing1}?', 'What does the Quran say about {thing2}?'),\n",
    "                thing1=quran, thing2=quran, remove_duplicates=True)\n",
    "test = Mft(data, labels=0)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bible', 'book', 'report', 'survey', 'poll', 'study', 'Constitution', 'data', 'bible', 'election', 'census', 'law', 'UN', 'Quran', 'evidence', 'science', 'constitution', 'Koran', 'vote', 'bill', 'film', 'Holocaust', 'research', 'Pope', 'letter', 'article', 'verdict', 'world', 'movie', 'government', 'video', 'Torah', 'song', 'moon', 'public', 'dictionary', 'president', 'referendum', 'ACA', 'eclipse', 'church', 'US', 'media', 'Church', 'pope', 'record', 'test', 'answer', 'literature', 'Prophet', 'CDC', 'FDA', 'WHO', 'scripture', 'community', 'Buddha', 'ban', 'prophet', 'statement', 'interview', 'Gospel', 'Book', 'past', 'question', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print([x[0][0] for x in tg.unmask_multiple(editor.template('What does the <mask> say about {thing}?', thing=quran))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1530 examples\n",
      "Test cases:     1530\n",
      "Failure rate:   8.9%\n",
      "\n",
      "Example fails:\n",
      "0.6 ('What does the Bible say about homosexuality?', 'What does the Church say about homosexuality?')\n",
      "1.0 ('What does the Gospel say about you?', 'What does the Bible say about you?')\n",
      "0.9 ('What does the Quran say about religion?', 'What does the Prophet say about religion?')\n",
      "0.6 ('What does the Bible say about women?', 'What does the Torah say about women?')\n",
      "1.0 ('What does the Bible say about God?', 'What does the Gospel say about God?')\n"
     ]
    }
   ],
   "source": [
    "books = ['Bible', 'Constitution', 'Quran', 'Pope', 'Torah', 'Church', 'Buddha', 'Prophet', 'Gospel', 'Book']\n",
    "data = editor.template(('What does the {book1} say about {thing}?', 'What does the {book2} say about {thing}?'),\n",
    "                thing=quran, book1=books, book2=books, remove_duplicates=True)\n",
    "test = Mft(data, labels=0)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inv, dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_map =  pickle.load(open('/home/marcotcr/tmp/processed_qqp.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_qs = [spacy_map[x[0]] for x in qs] + [spacy_map[x[1]] for x in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pairs = [(spacy_map[x[0]],spacy_map[x[1]]) for x in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(pair):\n",
    "    x0, x1 = pair\n",
    "    ents_0 = set([x[0].text for x in x0.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ents_1 = set([x[0].text for x in x1.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ret = []\n",
    "    if ents_0 and ents_0.intersection(ents_1):\n",
    "        for e in ents_0.intersection(ents_1):\n",
    "            if e == 'Quora':\n",
    "                continue\n",
    "            ret.extend([(pair[0].text.replace(e, n), pair[1].text.replace(e, n)) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        return [ret[i] for i in idxs]\n",
    "# change_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9792 examples\n",
      "Test cases:     1632\n",
      "Failure rate:   11.1%\n",
      "\n",
      "Example fails:\n",
      "0.2 (\"What is India's relationship with Bangladesh?\", \"What is Bangladesh's relationship with India?\")\n",
      "0.9 (\"What is India's relationship with Isabella?\", \"What is Isabella's relationship with India?\")\n",
      "0.9 (\"What is India's relationship with Jesus?\", \"What is Jesus's relationship with India?\")\n",
      "\n",
      "1.0 ('Which is best place to stay and visit in Kerala?', 'What are the best 10 places to visit in Kerala including any falls?')\n",
      "0.4 ('Which is best place to stay and visit in Sara?', 'What are the best 10 places to visit in Sara including any falls?')\n",
      "0.4 ('Which is best place to stay and visit in Emily?', 'What are the best 10 places to visit in Emily including any falls?')\n",
      "\n",
      "1.0 ('What is Jake Williams‚Äôs history that made him into a narcissist?', 'How is Jake Williams a narcissist?')\n",
      "0.3 ('What is Sara Williams‚Äôs history that made him into a narcissist?', 'How is Sara Williams a narcissist?')\n",
      "0.4 ('What is Nicole Williams‚Äôs history that made him into a narcissist?', 'How is Nicole Williams a narcissist?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = np.random.choice(len(processed_pairs), 2000,  replace=False)\n",
    "# idxs = np.random.choice(len(processed_pairs), len(processed_pairs),  replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_names, keep_original=True)\n",
    "# test = Dir(data, Expect.wrap(Expect.all(Expect.pairwise_to_group(Expect.monotonic_label(1, increasing=True, tolerance=0)))) )\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 432 examples\n",
      "Test cases:     72\n",
      "Failure rate:   5.6%\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What are the best places to visit in Wayanad, Kerala?', 'What can be the medium budget to visit best places in Kerala for three members (2-3 days)?')\n",
      "0.2 ('What are the best places to visit in Wayanad, Isaiah?', 'What can be the medium budget to visit best places in Isaiah for three members (2-3 days)?')\n",
      "0.0 ('What are the best places to visit in Wayanad, Jamie?', 'What can be the medium budget to visit best places in Jamie for three members (2-3 days)?')\n",
      "\n",
      "0.9 ('Was Donald Trump always rich?', 'How rich is Donald Trump?')\n",
      "0.5 ('Was Jason Trump always rich?', 'How rich is Jason Trump?')\n",
      "\n",
      "1.0 ('What is the best way to start contributing to the Linux kernel?', 'How should I start contributing for Linux?')\n",
      "0.2 ('What is the best way to start contributing to the Gabriel kernel?', 'How should I start contributing for Gabriel?')\n",
      "0.0 ('What is the best way to start contributing to the Elizabeth kernel?', 'How should I start contributing for Elizabeth?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idxs = np.random.choice(len(processed_pairs), 2000,  replace=False)\n",
    "# idxs = np.random.choice(len(processed_pairs), len(processed_pairs),  replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_names, keep_original=True)\n",
    "# test = Dir(data, Expect.wrap(Expect.all(Expect.pairwise_to_group(Expect.monotonic_label(1, increasing=True, tolerance=0)))) )\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.results.passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name_in_one(pair):\n",
    "    x0, x1 = pair\n",
    "    ents_0 = set([x[0].text for x in x0.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ents_1 = set([x[0].text for x in x1.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ret = []\n",
    "    if ents_0 and ents_0.intersection(ents_1):\n",
    "        for e in ents_0.intersection(ents_1):\n",
    "            if e == 'Quora':\n",
    "                continue\n",
    "            ret.extend([(pair[0].text.replace(e, n), pair[1].text) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "            ret.extend([(pair[0].text, pair[1].text.replace(e, n)) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        return [ret[i] for i in idxs]\n",
    "# change_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 756 examples\n",
      "Test cases:     126\n",
      "Filtered cases: 64 (50.8%)\n",
      "Failure rate:   3.2%\n",
      "\n",
      "Example fails:\n",
      "0.1 ('Why does Donald Trump think the debate schedule favors Hillary?', 'Why is Donald Trump saying the debate schedule is unfair? Is it because his support base would rather watch the NFL than his debate?')\n",
      "0.2 ('Why does Donald Trump think the debate schedule favors Hillary?', 'Why is Ava Trump saying the debate schedule is unfair? Is it because his support base would rather watch the NFL than his debate?')\n",
      "\n",
      "0.5 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Donald Trump winning the Republican nomination?')\n",
      "1.0 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Adam Trump winning the Republican nomination?')\n",
      "1.0 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Austin Trump winning the Republican nomination?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monotonic = Expect.pairwise(Expect.monotonic_label(1, increasing=False, tolerance=0.1))\n",
    "idxs = np.random.choice(len(processed_pairs), 3000, replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_name_in_one, keep_original=True)\n",
    "test = Dir(data, monotonic)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=False)\n",
    "test.summary(n=3)\n",
    "# test.results.passed[test.results.passed != None].astype(bool).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only consider examples that were duplicates\n",
    "sl_fn = lambda pred, *args, **kwargs: pred == 1\n",
    "# This substitutes the previous one\n",
    "# def sl_fn(x, pred, *args, **kwargs):\n",
    "#     print(pred)\n",
    "#     return pred[0] == 1\n",
    "sl_fn2 = lambda x, pred, *args, **kwargs: pred[0] == 1\n",
    "is_false = Expect.single(Expect.eq(0), agg_fn=Expect.all(ignore_first=True))#, slice_fn=sl_fn)\n",
    "# is_false = Expect.slice_pairwise(is_false, sl_fn)\n",
    "is_false = Expect.slice_testcase(is_false, sl_fn2)\n",
    "test.set_expect(is_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     126\n",
      "Filtered cases: 53 (42.1%)\n",
      "Failure rate:   52.1%\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What are the best books on Joseph Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "1.0 ('What are the best books on Peter Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "1.0 ('What are the best books on Jesse Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "\n",
      "1.0 ('How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sahara?', 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sonoran Desert?')\n",
      "1.0 ('How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sahara?', 'How cold can Tiffany Gobi Desert get, and how do its average temperatures compare to Tiffany ones in Tiffany Sonoran Desert?')\n",
      "1.0 ('How cold can Ella Gobi Desert get, and how do its average temperatures compare to Ella ones in Ella Sahara?', 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sonoran Desert?')\n",
      "\n",
      "0.9 ('Should I read A Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "0.9 ('Should I read Ian Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "0.9 ('Should I read Christina Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from checklist.inv_dir import pairwise_print_fn\n",
    "def fail_cr(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != 0\n",
    "def sort_cr(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return orig_conf[0]\n",
    "print_fn = pairwise_print_fn(fail_cr, sort_cr)\n",
    "test.summary(n=3, print_fn=print_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltests import bert_squad_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_squad_model.BertSquad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just makes confidence=1 for every prediction\n",
    "pp = PredictorWrapper.wrap_predict(model.predict_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2415e533d24776ac9f9d70c9e2c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mary']"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_pairs([('Who is dumb?', 'Mary is somewhat dumb. John is not so dumb.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f75826c94941a5a642cf712e25dfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Mary'], array([1.]))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp([('Who is dumb?', 'Mary is somewhat dumb. John is not so dumb.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drinking', 'smoking', 'crying', 'reading', 'writing', 'worrying', 'eating', 'driving', 'listening', 'praying', 'talking', 'working', 'trying', 'calling', 'laughing']\n"
     ]
    }
   ],
   "source": [
    "verbs = [x[0][0] for x in tg.unmask('Luke told Mary that she should probably stop <mask>.')]\n",
    "verbs = [x for x in verbs if 'ing' in x]\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data, maps = editor.template(('Who is currently {verb}?', '{male} told {female} she should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['female'] for m in maps]\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{male} told {female} he should probably stop {verb}'), nsamples=100,verb=verbs, return_maps=True)\n",
    "labels += [m['male'] for m in maps]\n",
    "data += n\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{female} told {male} he should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['male'] for m in maps]\n",
    "data += n\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{female} told {male} she should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['female'] for m in maps]\n",
    "data += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Who is currently worrying?',\n",
       " 'Juan told Crystal she should probably stop worrying')"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f8fa8a43d1438cbc766449fd7abdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = Mft(data, labels=labels)\n",
    "test.run(pp)\n",
    "# test.results.passed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):\n",
    "    q, c = x\n",
    "    ret = 'C: %s\\nQ: %s\\n' % (c, q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad(x, pred, conf, label=None, *args, **kwargs):\n",
    "    q, c = x\n",
    "    ret = 'Q: %s\\n' % (q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     400\n",
      "Failure rate:   58.2%\n",
      "\n",
      "Example fails:\n",
      "C: Taylor told Carlos she should probably stop praying\n",
      "Q: Who is currently praying?\n",
      "A: Taylor\n",
      "P: Carlos\n",
      "\n",
      "C: Carlos told Rachel he should probably stop worrying\n",
      "Q: Who is currently worrying?\n",
      "A: Carlos\n",
      "P: Rachel\n",
      "\n",
      "C: Peter told Chloe she should probably stop trying\n",
      "Q: Who is currently trying?\n",
      "A: Chloe\n",
      "P: Peter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3, format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked = [x[0][0] for x in tg.unmask('John was told by Luke that he really likes <mask>.', beam_size=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data, maps = editor.template(('Who really likes {liked}?', '{male1} was told by {male2} that {male2} really likes {liked}.'), male1=editor.lexicons['male'], male2=editor.lexicons['male'], nsamples=100, verb=verbs, liked=liked, return_maps=True, remove_duplicates=True)\n",
    "labels += [m['male2'] for m in maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a2d7705454f35b5de65d29223020a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test cases:     100\n",
      "Failure rate:   44.0%\n",
      "\n",
      "Example fails:\n",
      "C: Eric was told by Austin that Austin really likes Hunter.\n",
      "Q: Who really likes Hunter?\n",
      "A: Austin\n",
      "P: Eric\n",
      "\n",
      "C: Ethan was told by Juan that Juan really likes Jess.\n",
      "Q: Who really likes Jess?\n",
      "A: Juan\n",
      "P: Juan that Juan\n",
      "\n",
      "C: Robert was told by Ian that Ian really likes Vanessa.\n",
      "Q: Who really likes Vanessa?\n",
      "A: Ian\n",
      "P: Robert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = Mft(data, labels=labels)\n",
    "test.run(pp)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bible', 'book', 'report', 'survey', 'poll', 'study', 'Constitution', 'data', 'bible', 'election', 'census', 'law', 'UN', 'Quran', 'evidence', 'science', 'constitution', 'Koran', 'vote', 'bill', 'film', 'Holocaust', 'research', 'Pope', 'letter', 'article', 'verdict', 'world', 'movie', 'government', 'video', 'Torah', 'song', 'moon', 'public', 'dictionary', 'president', 'referendum', 'ACA', 'eclipse', 'church', 'US', 'media', 'Church', 'pope', 'record', 'test', 'answer', 'literature', 'Prophet', 'CDC', 'FDA', 'WHO', 'scripture', 'community', 'Buddha', 'ban', 'prophet', 'statement', 'interview', 'Gospel', 'Book', 'past', 'question', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print([x[0][0] for x in tg.unmask_multiple(editor.template('What does the <mask> say about {thing}?', thing=quran))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_squad(fold='validation'):\n",
    "    answers = []\n",
    "    data = []\n",
    "    ids = []\n",
    "    files = {\n",
    "        'validation': '/home/marcotcr/datasets/squad/dev-v1.1.json',\n",
    "        'train': '/home/marcotcr//datasets/squad/train-v1.1.json',\n",
    "        }\n",
    "    f = json.load(open(files[fold]))\n",
    "    for t in f['data']:\n",
    "        for p in t['paragraphs']:\n",
    "            context = p['context']\n",
    "            for qa in p['qas']:\n",
    "                data.append({'passage': context, 'question': qa['question'], 'id': qa['id']})\n",
    "                answers.append(set([(x['text'], x['answer_start']) for x in qa['answers']]))\n",
    "    return data, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, answers =  load_squad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(x['question'], x['passage']) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(x):\n",
    "    q, c = x\n",
    "    in_p = set()\n",
    "    not_in_p = set()\n",
    "    for n in editor.lexicons['male'][:10]:\n",
    "        if re.search(r'\\b%s\\b' % n, c):\n",
    "            in_p.add(n)\n",
    "        else:\n",
    "            not_in_p.add(n)\n",
    "    if not in_p:\n",
    "        return None\n",
    "    ret = []\n",
    "    ret_add = []\n",
    "    for p in in_p:\n",
    "        for n in not_in_p:\n",
    "            ret.append((re.sub(r'\\b%s\\b' % p, n, q), re.sub(r'\\b%s\\b' % p, n, c)))\n",
    "            ret_add.append((p, n))\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        ret = [ret[i] for i in idxs]\n",
    "        ret_add = [ret_add[i] for i in idxs]\n",
    "    return ret, ret_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i, x in enumerate(pairs) if 'John' in x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2058 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37654cb9177d49eca96387bc6ffd2792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=268.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test cases:     343\n",
      "Failure rate:   1.2%\n",
      "\n",
      "Example fails:\n",
      "Q: What did John Dobson describe Newcastle as?\n",
      "P: neoclassical centre referred to as Tyneside Classical\n",
      "\n",
      "John -> Daniel\n",
      "Q: What did Daniel Dobson describe Newcastle as?\n",
      "P: neoclassical\n",
      "\n",
      "\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor\n",
      "\n",
      "John -> James\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor meets an unknown incarnation of himself\n",
      "\n",
      "John -> Joseph\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor meets an unknown incarnation of himself\n",
      "\n",
      "\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: both Kenia and Kegnia\n",
      "\n",
      "Joseph -> James\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: Kenia and Kegnia\n",
      "\n",
      "Joseph -> William\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: Kenia and Kegnia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def new_eq(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    if meta:\n",
    "        p, n = meta\n",
    "        orig_pred =  re.sub(r'\\b%s\\b' % p, n, orig_pred)\n",
    "    return pred == orig_pred\n",
    "\n",
    "def format_name(x, pred, conf, label=None, meta=None):\n",
    "    ret = ''\n",
    "    if meta is not None and len(meta):\n",
    "        ret = '%s -> %s\\n' % meta\n",
    "    ret += format_squad(x, pred, conf, label, meta)\n",
    "    return ret\n",
    "    \n",
    "# tt = Expect.wrap(Expect.all(Expect.pairwise_to_group(new_eq), ignore_first=True))\n",
    "random_idxs = np.random.choice(len(pairs), 3000)\n",
    "data, meta = Perturb.perturb([pairs[i] for i in random_idxs], change_name, returns_additional=True)\n",
    "tt = Expect.pairwise(new_eq)\n",
    "test = Inv(data, expect=tt, meta=meta)\n",
    "test.run(pp)\n",
    "test.summary(n=3, format_example_fn=format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " ('John', 'Joshua'),\n",
       " ('John', 'Matthew'),\n",
       " ('John', 'William'),\n",
       " ('John', 'Christopher'),\n",
       " ('John', 'Michael'),\n",
       " ('John', 'James'),\n",
       " ('John', 'David'),\n",
       " ('John', 'Daniel'),\n",
       " ('John', 'Joseph')]"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['John Fox', 'Joshua Fox', 'Matthew Fox', 'William Fox',\n",
       "       'Christopher Fox', 'Michael Fox', 'James Fox', 'David Fox',\n",
       "       'Daniel Fox', 'Joseph Fox'], dtype='<U15')"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.results.preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function checklist.expect.Expect.all.<locals>.expect(xs, preds, confs, labels=None, meta=None)>"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John', 'Joshua')"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_name(pairs[178])[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in pairs if 'John' in x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "How is vanilla extract made?"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_qs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Luke', 'Mark']"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons['male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40430"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NORP'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_qs[0].ents[0][0].ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template('This is {bad}',  bad=['bad', 'great', 'awesome'], return_maps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('This is bad', 'This is not bad'),\n",
       "  ('This is great', 'This is not great'),\n",
       "  ('This is awesome', 'This is not awesome')],\n",
       " [{'bad': 'bad'}, {'bad': 'great'}, {'bad': 'awesome'}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "editor.template(('This is {bad}', 'This is not {bad}'),  bad=['bad', 'great', 'awesome'], return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bad': 'This is bad', 'notbad': 'This is not bad'},\n",
       " {'bad': 'This is great', 'notbad': 'This is not great'},\n",
       " {'bad': 'This is awesome', 'notbad': 'This is not awesome'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template({\n",
    "    'bad': 'This is {bad}',\n",
    "    'notbad': 'This is not {bad}'},  bad=['bad', 'great', 'awesome'], return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'bad': 'This is {bad}',\n",
    "    'notbad': ('this is not {bad}', 'this is quite {abad}')}\n",
    "b = ({'bad': 'This is {bad}', 'notbad': 'This is not {bad}'}, 'This is quite {bad}')\n",
    "c = 'This is quite {bad}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': ['John', 'Luke', 'Mark'], 'female': ['Mary', 'Judy', 'Julia']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is terrible, John',\n",
       " 'This is bad, John',\n",
       " 'This is terrible, Luke',\n",
       " 'This is bad, Luke',\n",
       " 'This is terrible, Mark',\n",
       " 'This is bad, Mark']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is {bad}, {male}', bad=('terrible', 'bad'), nsamples=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

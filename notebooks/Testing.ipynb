{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "import spacy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.mft import Mft\n",
    "from checklist.inv_dir import Inv, Dir\n",
    "from checklist.expect import Expect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg = checklist.text_generation.TextGenerator(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/marcotcr/work/ml-tests/')\n",
    "from mltests import model_wrapper\n",
    "sentiment = model_wrapper.ModelWrapper()\n",
    "pp = PredictorWrapper.wrap_softmax(sentiment.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg.unmask_multiple(['I really <mask> the pilot.', 'I really <mask> the flight.'], metric='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['female_names', 'male_names', 'names', 'last_names', 'countries', 'nationalities', 'cities', 'sexuality_adjs', 'religions', 'religion_adjs', 'verbs_3ps', 'verbs_3pp', 'nouns', 'adjs'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "editor = checklist.editor.Editor()\n",
    "c = pickle.load(open('/home/marcotcr/work/ml-tests/data/common.pkl', 'rb'))\n",
    "editor.lexicons['male'] = c['male_names']\n",
    "editor.lexicons['female'] = c['female_names']\n",
    "c.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b2b713fc6478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is not {bert}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'This is not {bert} bad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/checklist/checklist/editor.py\u001b[0m in \u001b[0;36mtemplate\u001b[0;34m(self, templates, return_meta, nsamples, product, remove_duplicates, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# print(ts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m# print(samp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmask_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/checklist/checklist/editor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# print(ts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m# print(samp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmask_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "editor.template(('This is not {bert}', 'This is not {bert} bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is not VERYLONGTOKENTHATWILLNOTEXISTEVER VERYLONGTOKENTHATWILLNOTEXISTEVER', 'This is not VERYLONGTOKENTHATWILLNOTEXISTEVER bad')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3b009b295e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is not {bert} {bert}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'This is not {bert} bad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/checklist/checklist/editor.py\u001b[0m in \u001b[0;36mtemplate\u001b[0;34m(self, templates, return_meta, nsamples, product, remove_duplicates, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m# self.tg.unmask_multiple(samp,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "editor.template(('This is not {bert} {bert}', 'This is not {bert} bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat dog dog'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('dog', 'cat', 'dog dog dog', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is not {bert[0]} {bert[1]} {bert1[2]} {bert2[3]}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklist.editor.replace_bert('This is not {bert} {bert} {bert1} {bert2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = re.compile(r'\\{bert\\d*\\}')\n",
    "a.findall('This is not {bert} {bert1}, {bert342}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Can only have one bert index per template string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-ba59f54f1490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchecklist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bert_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is not {besrt} {abert} {bert} {bert2}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/checklist/checklist/editor.py\u001b[0m in \u001b[0;36mget_bert_index\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mberts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mberts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Can only have one bert index per template string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mberts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbert_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mberts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Can only have one bert index per template string"
     ]
    }
   ],
   "source": [
    "checklist.editor.get_bert_index(('This is not {besrt} {abert} {bert} {bert2}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function checklist.editor.bert_index.<locals>.<lambda>()>,\n",
       "            {'bert': ['This is not {bert} {bert}', 'This is not {bert} bad'],\n",
       "             'bert1': ['This is not {bert1} s']})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklist.editor.bert_index(('This is not {bert} {bert}', 'This is not {bert} bad', 'This is not {bert1} s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is not a1 a2.', 'This is not b1 b2.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is not {b[0]} {b[1]}.', b=[('a1', 'a2'), ('b1', 'b2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = editor.tg.unmask_multiple(['This is a <mask>.', 'This is a <mask> <mask>.'], beam_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['short', 'version'], ' This is a short version.', 13.087730884552002)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.tg.unmask_multiple(['This is <mask>.', 'This is an <mask> <mask>.'], beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'str1': 'This is not the worst thing in the campus',\n",
       " 'str2': 'I think this is the worst place in the campus',\n",
       " 'str3': 'He is a good movie.'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['good'], ' This is not a very good movie.', 18.80685806274414),\n",
       " (['nice'], ' This is not a very nice movie.', 15.89453125),\n",
       " (['funny'], ' This is not a very funny movie.', 15.101058959960938),\n",
       " (['bad'], ' This is not a very bad movie.', 14.818763732910156),\n",
       " (['great'], ' This is not a very great movie.', 14.58511734008789),\n",
       " (['interesting'],\n",
       "  ' This is not a very interesting movie.',\n",
       "  14.357146263122559),\n",
       " (['fun'], ' This is not a very fun movie.', 14.004464149475098),\n",
       " (['exciting'], ' This is not a very exciting movie.', 13.874397277832031),\n",
       " (['pleasant'], ' This is not a very pleasant movie.', 13.860466003417969),\n",
       " (['long'], ' This is not a very long movie.', 13.80008316040039),\n",
       " (['violent'], ' This is not a very violent movie.', 13.722796440124512),\n",
       " (['entertaining'],\n",
       "  ' This is not a very entertaining movie.',\n",
       "  13.66596794128418),\n",
       " (['cool'], ' This is not a very cool movie.', 13.545897483825684),\n",
       " (['smart'], ' This is not a very smart movie.', 13.475765228271484),\n",
       " (['happy'], ' This is not a very happy movie.', 13.210590362548828),\n",
       " (['successful'], ' This is not a very successful movie.', 13.141807556152344),\n",
       " (['enjoyable'], ' This is not a very enjoyable movie.', 13.135686874389648),\n",
       " (['popular'], ' This is not a very popular movie.', 13.096426010131836),\n",
       " (['pretty'], ' This is not a very pretty movie.', 13.07435131072998),\n",
       " (['inspiring'], ' This is not a very inspiring movie.', 12.896029472351074)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.tg.unmask('This is not a very <mask> movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['player'], ' John is a very good player', 12.612285614013672),\n",
       " (['game'], ' John is a very good game', 12.206633567810059),\n",
       " (['book'], ' John is a very good book', 11.615123748779297),\n",
       " (['writer'], ' John is a very good writer', 11.266648292541504),\n",
       " (['example'], ' John is a very good example', 11.174124717712402),\n",
       " (['guy'], ' John is a very good guy', 11.041021347045898),\n",
       " (['read'], ' John is a very good read', 10.992713928222656),\n",
       " (['question'], ' John is a very good question', 10.974016189575195),\n",
       " (['pick'], ' John is a very good pick', 10.947866439819336),\n",
       " (['shooter'], ' John is a very good shooter', 10.893823623657227),\n",
       " (['story'], ' John is a very good story', 10.848344802856445),\n",
       " (['article'], ' John is a very good article', 10.771170616149902),\n",
       " (['candidate'], ' John is a very good candidate', 10.673225402832031),\n",
       " (['play'], ' John is a very good play', 10.646265029907227),\n",
       " (['idea'], ' John is a very good idea', 10.545669555664062),\n",
       " (['film'], ' John is a very good film', 10.3023681640625),\n",
       " (['coach'], ' John is a very good coach', 10.297456741333008),\n",
       " (['student'], ' John is a very good student', 10.237459182739258),\n",
       " (['list'], ' John is a very good list', 10.21300220489502),\n",
       " (['job'], ' John is a very good job', 10.210472106933594)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.tg.unmask('John is a very good <mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is {a:thing} of {a:god}'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "f = string.Formatter()\n",
    "s = 'This is {a:thing} of {a:god}'\n",
    "list(f.parse(s))\n",
    "checklist.editor.remove_options(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is {thing} of {god}'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sub(r'{\\1}', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing', 'god']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "m = re.compile(r'\\{[^\\}]+:([^\\}]*)\\}')\n",
    "m.findall(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a mna'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_option = re.compile(r'.*:')\n",
    "remove_option.sub('', 'afsdf:this is a mna')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function checklist.editor.get_bert_index.<locals>.<lambda>()>,\n",
       "            {'bert': ['This is {a:bert} {a:bert}.']})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_finder = re.compile(r'\\{(?:[^\\}]*:)?bert\\d*\\}')\n",
    "x = 'This is {a:bert} {a:bert}.'\n",
    "checklist.editor.get_bert_index(x)\n",
    "# bert_finder.findall(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is {fsdfasfasd} {bert} {fsdfasfasd} {bert}.'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'{a:([^}]*)}', r'{fsdfasfasd} {\\1}', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a bad man.',\n",
       " 'This is an incredible show.',\n",
       " 'This is a great person.',\n",
       " 'This is an important person.',\n",
       " 'This is an interesting person.']"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is {a:bert} {thing}.', thing=['movie', 'show', 'person', 'man'], nsamples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'Samuel is not a feminist, Christine is.',\n",
       "  'question': 'Who is a feminist?',\n",
       "  'answer': 'Christine'},\n",
       " {'context': 'Stephen is not an adult, Ava is.',\n",
       "  'question': 'Who is an adult?',\n",
       "  'answer': 'Ava'},\n",
       " {'context': 'John is not an atheist, Angela is.',\n",
       "  'question': 'Who is an atheist?',\n",
       "  'answer': 'Angela'},\n",
       " {'context': 'Ethan is not a terrorist, Brooke is.',\n",
       "  'question': 'Who is a terrorist?',\n",
       "  'answer': 'Brooke'},\n",
       " {'context': 'Robert is not an adult, Sarah is.',\n",
       "  'question': 'Who is an adult?',\n",
       "  'answer': 'Sarah'}]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template({\n",
    "   'context': '{male} is not {a:bert}, {female} is.' ,\n",
    "   'question': 'Who is {a:bert}?',\n",
    "   'answer': '{female}'\n",
    "    }, nsamples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['This is a work in progress.',\n",
       "  'This is a work in Progress.',\n",
       "  'This is a work of art.',\n",
       "  'This is a work in process.',\n",
       "  'This is a work in art.',\n",
       "  'This is a work in development.',\n",
       "  'This is a work of fiction.',\n",
       "  'This is a work in fiction.',\n",
       "  'This is an abbrevi ated version.',\n",
       "  'This is a work in motion.'],\n",
       " [{'bert': ['work', 'in', 'progress']},\n",
       "  {'bert': ['work', 'in', 'Progress']},\n",
       "  {'bert': ['work', 'of', 'art']},\n",
       "  {'bert': ['work', 'in', 'process']},\n",
       "  {'bert': ['work', 'in', 'art']},\n",
       "  {'bert': ['work', 'in', 'development']},\n",
       "  {'bert': ['work', 'of', 'fiction']},\n",
       "  {'bert': ['work', 'in', 'fiction']},\n",
       "  {'bert': ['abbrevi', 'ated', 'version']},\n",
       "  {'bert': ['work', 'in', 'motion']}])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is {a:bert} {bert} {bert}.', return_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a {thing} {thing}'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {'thing' : 'man'}\n",
    "x = 'This is {a:thing} {b:thing}'\n",
    "a = re.compile(r'{([^}]+):([^}]+)}')\n",
    "# 'This is {a:thing}'.format(**mapping)\n",
    "a.search(x).group(1,2)\n",
    "def add_article(noun):\n",
    "    return 'an %s' % noun if noun[0].lower() in ['a', 'e', 'i', 'o', 'u'] else 'a %s' % noun\n",
    "\n",
    "def mysub(match):\n",
    "    options, thing = match.group(1, 2)\n",
    "    ret = '' \n",
    "    if 'a' in options:\n",
    "        ret += '%s ' % add_article(thing).split()[0]\n",
    "    ret += '{%s}' % thing\n",
    "    return ret\n",
    "a.sub(mysub, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This is a book', 'book'), ('This is an aurora', 'aurora')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template(('This is {a:thing}', '{thing}'), thing=['book', 'aurora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a very good book.',\n",
       " 'This is a very interesting book.',\n",
       " 'This is a very nice book.',\n",
       " 'This is a very difficult book.',\n",
       " 'This is a very important book.',\n",
       " 'This is a very strange book.',\n",
       " 'This is a very powerful book.',\n",
       " 'This is a very special book.',\n",
       " 'This is a very smart book.',\n",
       " 'This is a very intelligent book.',\n",
       " 'This is a very good movie.',\n",
       " 'This is a very interesting movie.',\n",
       " 'This is a very nice movie.',\n",
       " 'This is a very difficult movie.',\n",
       " 'This is a very important movie.',\n",
       " 'This is a very strange movie.',\n",
       " 'This is a very powerful movie.',\n",
       " 'This is a very special movie.',\n",
       " 'This is a very smart movie.',\n",
       " 'This is a very intelligent movie.',\n",
       " 'This is a very good person.',\n",
       " 'This is a very interesting person.',\n",
       " 'This is a very nice person.',\n",
       " 'This is a very difficult person.',\n",
       " 'This is a very important person.',\n",
       " 'This is a very strange person.',\n",
       " 'This is a very powerful person.',\n",
       " 'This is a very special person.',\n",
       " 'This is a very smart person.',\n",
       " 'This is a very intelligent person.',\n",
       " 'This is a very good student.',\n",
       " 'This is a very interesting student.',\n",
       " 'This is a very nice student.',\n",
       " 'This is a very difficult student.',\n",
       " 'This is a very important student.',\n",
       " 'This is a very strange student.',\n",
       " 'This is a very powerful student.',\n",
       " 'This is a very special student.',\n",
       " 'This is a very smart student.',\n",
       " 'This is a very intelligent student.']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is a very {bert} {thing}.', thing=['book', 'movie', 'person', 'student'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jordan has a very good game plan.',\n",
       " 'Patrick has a very good offensive line.',\n",
       " 'Brian has a very good track record.',\n",
       " 'Edward has a very good offensive game.',\n",
       " 'Brian has a very good track record.',\n",
       " 'Zachary has a very good work rate.',\n",
       " 'Jayden has a very good offensive system.',\n",
       " 'Jared has a very good track record.',\n",
       " 'Jesse has a very good offensive system.',\n",
       " 'David has a very good offensive game.',\n",
       " 'Jonathan has a very good offensive game.',\n",
       " 'Logan has a very good skill base.',\n",
       " 'Alexander has a very good track record.',\n",
       " 'Kyle has a very good chess hand.',\n",
       " 'Nathaniel has a very good offensive line.',\n",
       " 'Joshua has a very good skill base.',\n",
       " 'Jared has a very good track record.',\n",
       " 'Alex has a very good offensive game.',\n",
       " 'Nathaniel has a very good offensive game.',\n",
       " 'Aaron has a very good skill base.',\n",
       " 'Jeremy has a very good work rate.',\n",
       " 'Brandon has a very good skill set.',\n",
       " 'Jared has a very good track record.',\n",
       " 'Edward has a very good offensive line.',\n",
       " 'Elijah has a very good offensive line.',\n",
       " 'David has a very good track record.',\n",
       " 'Alexander has a very good track pedigree.',\n",
       " 'Luke has a very good chess hand.',\n",
       " 'Edward has a very good offensive game.',\n",
       " 'Nathaniel has a very good offensive game.',\n",
       " 'Jack has a very good offensive line.',\n",
       " 'Nicholas has a very good track record.',\n",
       " 'Paul has a very good game plan.',\n",
       " 'Peter has a very good chess hand.',\n",
       " 'Alex has a very good chess hand.',\n",
       " 'Jared has a very good track record.',\n",
       " 'Mason has a very good game plan.',\n",
       " 'Nicholas has a very good chess hand.',\n",
       " 'Samuel has a very good chess hand.',\n",
       " 'Alex has a very good offensive line.',\n",
       " 'Brandon has a very good track record.',\n",
       " 'Tyler has a very good offensive system.',\n",
       " 'Samuel has a very good track pedigree.',\n",
       " 'Shawn has a very good track pedigree.',\n",
       " 'Kyle has a very good game plan.',\n",
       " 'Christopher has a very good game plan.',\n",
       " 'Peter has a very good skill base.',\n",
       " 'Jacob has a very good track pedigree.',\n",
       " 'Kevin has a very good work rate.',\n",
       " 'Jeremy has a very good game plan.',\n",
       " 'Mark has a very good offensive system.',\n",
       " 'Aiden has a very good chess hand.',\n",
       " 'Carlos has a very good offensive line.',\n",
       " 'Ethan has a very good offensive game.',\n",
       " 'Ethan has a very good offensive system.',\n",
       " 'Mark has a very good chess hand.',\n",
       " 'Sean has a very good track record.',\n",
       " 'Adam has a very good track pedigree.',\n",
       " 'Isaiah has a very good work rate.',\n",
       " 'Edward has a very good work rate.',\n",
       " 'Joseph has a very good track pedigree.',\n",
       " 'Ryan has a very good track pedigree.',\n",
       " 'Jack has a very good skill base.',\n",
       " 'Michael has a very good track record.',\n",
       " 'Robert has a very good track record.',\n",
       " 'Nathan has a very good offensive system.',\n",
       " 'Kenneth has a very good skill set.',\n",
       " 'Hunter has a very good track pedigree.',\n",
       " 'Stephen has a very good skill set.',\n",
       " 'Nathan has a very good skill base.',\n",
       " 'Gabriel has a very good offensive line.',\n",
       " 'Jack has a very good game plan.',\n",
       " 'Joseph has a very good skill set.',\n",
       " 'Noah has a very good work rate.',\n",
       " 'Jordan has a very good work rate.',\n",
       " 'Jesse has a very good track pedigree.',\n",
       " 'Gabriel has a very good work rate.',\n",
       " 'Ian has a very good game plan.',\n",
       " 'Charles has a very good track pedigree.',\n",
       " 'Julian has a very good skill base.',\n",
       " 'Christian has a very good work rate.',\n",
       " 'Isaiah has a very good work rate.',\n",
       " 'Scott has a very good skill base.',\n",
       " 'Jeremy has a very good offensive line.',\n",
       " 'Cody has a very good track record.',\n",
       " 'Jonathan has a very good game plan.',\n",
       " 'Hunter has a very good game plan.',\n",
       " 'Jesse has a very good offensive game.',\n",
       " 'John has a very good skill set.',\n",
       " 'Kevin has a very good track record.',\n",
       " 'Patrick has a very good offensive system.',\n",
       " 'Julian has a very good skill base.',\n",
       " 'Adam has a very good chess hand.',\n",
       " 'Christian has a very good game plan.',\n",
       " 'Shawn has a very good offensive system.',\n",
       " 'Christopher has a very good offensive game.',\n",
       " 'Jackson has a very good offensive line.',\n",
       " 'Luis has a very good game plan.',\n",
       " 'Peter has a very good skill base.',\n",
       " 'John has a very good skill set.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('{male} has a very good {bert} {bert}.', thing=['movie', 'book', 'tv show', 'person', 'dog', 'country'], nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['good'], ' This is a good movie.', 15.688888549804688),\n",
       " (['great'], ' This is a great movie.', 14.89714241027832),\n",
       " (['bad'], ' This is a bad movie.', 14.015711784362793),\n",
       " (['terrible'], ' This is a terrible movie.', 13.26148796081543),\n",
       " (['nice'], ' This is a nice movie.', 12.802513122558594),\n",
       " (['horror'], ' This is a horror movie.', 12.745840072631836),\n",
       " (['scary'], ' This is a scary movie.', 12.734757423400879),\n",
       " (['sad'], ' This is a sad movie.', 12.687625885009766),\n",
       " (['beautiful'], ' This is a beautiful movie.', 12.429882049560547),\n",
       " (['funny'], ' This is a funny movie.', 12.342477798461914),\n",
       " (['wonderful'], ' This is a wonderful movie.', 12.230314254760742),\n",
       " (['short'], ' This is a short movie.', 12.098125457763672),\n",
       " (['cult'], ' This is a cult movie.', 12.094284057617188),\n",
       " (['fun'], ' This is a fun movie.', 11.956672668457031),\n",
       " (['horrible'], ' This is a horrible movie.', 11.884394645690918),\n",
       " (['fantastic'], ' This is a fantastic movie.', 11.882230758666992),\n",
       " (['weird'], ' This is a weird movie.', 11.824714660644531),\n",
       " (['violent'], ' This is a violent movie.', 11.688575744628906),\n",
       " (['stupid'], ' This is a stupid movie.', 11.66788387298584),\n",
       " (['cool'], ' This is a cool movie.', 11.60772705078125)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.tg.unmask('This is a <mask> movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is not a very good movie',\n",
       " 'This is not a very nice movie',\n",
       " 'This is not a very funny movie',\n",
       " 'This is not a very bad movie',\n",
       " 'This is not a very great movie',\n",
       " 'This is not a very violent movie',\n",
       " 'This is not a very fun movie',\n",
       " 'This is not a very long movie',\n",
       " 'This is not a very interesting movie',\n",
       " 'This is not a very smart movie']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is not a very {bert} {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'str1': 'This is not the best restaurant in the world',\n",
       "  'str2': 'I think this is the best place in the world',\n",
       "  'str3': 'This is a horrible movie.',\n",
       "  'str4': \"This movie is not good, it's horrible\"},\n",
       " {'str1': 'This is not the best shot in the campus',\n",
       "  'str2': 'I think this is the best place in the campus',\n",
       "  'str3': 'This is a stupid movie.',\n",
       "  'str4': \"This movie is not good, it's stupid\"},\n",
       " {'str1': 'This is not the best place in the library',\n",
       "  'str2': 'I think this is the best place in the library',\n",
       "  'str3': 'This is a bad movie.',\n",
       "  'str4': \"This movie is not good, it's bad\"},\n",
       " {'str1': 'This is not the worst thing in the world',\n",
       "  'str2': 'I think this is the worst place in the world',\n",
       "  'str3': 'This is a awful movie.',\n",
       "  'str4': \"This movie is not good, it's awful\"},\n",
       " {'str1': 'This is not the best view in the place',\n",
       "  'str2': 'I think this is the best place in the place',\n",
       "  'str3': 'This is a crap movie.',\n",
       "  'str4': \"This movie is not good, it's crap\"}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = editor.template({'str1': 'This is not the {bert} {bert} in the {a}',\n",
    "                 'str2': 'I think this is the {bert} place in the {a}', \n",
    "                 'str3': 'This is a {bert1} movie.',\n",
    "                 'str4': 'This movie is not good, it\\'s {bert1}'},\n",
    "                a=['world', 'library', 'campus', 'place'], return_meta=False, nsamples=500)\n",
    "a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is not the safest place in the library',\n",
       " 'This is not the worst place in the place',\n",
       " 'This is not the only one in the library',\n",
       " 'This is not the safest room in the world',\n",
       " 'This is not the best place in the campus',\n",
       " 'This is not the worst place in the world',\n",
       " 'This is not the worst thing in the world',\n",
       " 'This is not the safest room in the world',\n",
       " 'This is not the worst thing in the campus',\n",
       " 'This is not the safest spot in the world',\n",
       " 'This is not the safest spot in the campus',\n",
       " 'This is not the safest place in the campus',\n",
       " 'This is not the biggest thing in the world',\n",
       " 'This is not the best place in the library',\n",
       " 'This is not the worst thing in the campus',\n",
       " 'This is not the biggest problem in the place',\n",
       " 'This is not the biggest problem in the place',\n",
       " 'This is not the only one in the world',\n",
       " 'This is not the only problem in the campus',\n",
       " 'This is not the best place in the campus']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is not the {bert} {bert} in the {a}',  a=['world', 'library', 'campus', 'place'], return_meta=False, nsamples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template('This is not BERT bad {a}.',a=['a', 'b'], return_meta=False, nsamples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is VERYLONGTOKENTHATWILLNOTEXISTEVER not VERYLONGTOKENTHATWILLNOTEXISTEVER VERYLONGTOKENTHATWILLNOTEXISTEVER bad.']\n",
      "['This is VERYLONGTOKENTHATWILLNOTEXISTEVER not VERYLONGTOKENTHATWILLNOTEXISTEVER VERYLONGTOKENTHATWILLNOTEXISTEVER bad.']\n",
      "['This is <mask> not <mask> <mask> bad.']\n",
      "aefjdksalf\n",
      "[['really', 'all', 'that'], ['actually', 'all', 'that'], ['probably', 'all', 'that'], ['honestly', 'all', 'that'], ['still', 'all', 'that'], ['really', 'half', '-'], ['actually', 'half', '-'], ['actually', 'nearly', 'as'], ['actually', 'half', 'as'], ['actually', 'quite', 'so']]\n",
      "This is {bert[0]} not {bert[1]} {bert[2]} bad.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['This is really not all that bad.',\n",
       "  'This is actually not all that bad.',\n",
       "  'This is probably not all that bad.',\n",
       "  'This is honestly not all that bad.',\n",
       "  'This is still not all that bad.',\n",
       "  'This is really not half - bad.',\n",
       "  'This is actually not half - bad.',\n",
       "  'This is actually not nearly as bad.',\n",
       "  'This is actually not half as bad.',\n",
       "  'This is actually not quite so bad.'],\n",
       " [{'bert': ['really', 'all', 'that']},\n",
       "  {'bert': ['actually', 'all', 'that']},\n",
       "  {'bert': ['probably', 'all', 'that']},\n",
       "  {'bert': ['honestly', 'all', 'that']},\n",
       "  {'bert': ['still', 'all', 'that']},\n",
       "  {'bert': ['really', 'half', '-']},\n",
       "  {'bert': ['actually', 'half', '-']},\n",
       "  {'bert': ['actually', 'nearly', 'as']},\n",
       "  {'bert': ['actually', 'half', 'as']},\n",
       "  {'bert': ['actually', 'quite', 'so']}])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is {bert} not {bert} {bert} bad.', return_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = ['flight', 'seat', 'pilot', 'staff', 'plane', 'airline', 'cabin crew', 'aircraft', 'food']\n",
    "pos = ['liked', 'enjoyed', 'loved', 'admired', 'appreciated']\n",
    "neg = ['hated', 'disliked', 'regretted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-c38380a635c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I really <mask> the {n}.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmask_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tg.unmask_multiple(ts, metric='avg', beam_size=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tg' is not defined"
     ]
    }
   ],
   "source": [
    "ts = editor.template('I really <mask> the {n}.', n=nouns)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000)][:20])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'We', 'He', 'he', 'She', 'They', 'we', 'and', '...', 'she', 'they', 'Everyone', 'i', 'People', 'You', 'Alex', 'Everybody', 'And', 'Dad', 'John']\n"
     ]
    }
   ],
   "source": [
    "ts = editor.template('<mask> really {pn} the {n}.', n=nouns, pn=pos+neg)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000)][:20])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = ['I', 'We', 'They', 'we', 'He', 'he', 'She', 'she', 'they', 'people', 'People', 'you', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-31178ff2f6d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I didn\\'t like the {n}, it was very <mask>.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmask_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tg.unmask_multiple(ts, metric='avg', beam_size=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tg' is not defined"
     ]
    }
   ],
   "source": [
    "ts = editor.template('I didn\\'t like the {n}, it was very <mask>.', n=nouns)\n",
    "print([x[0][0] for x in tg.unmask_multiple(ts, beam_size=1000, metric='avg')][:100])\n",
    "# tg.unmask_multiple(ts, metric='avg', beam_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj = ['good', 'nice', 'helpful', 'comfortable', 'cool', 'efficient', 'pleasant', 'interesting', 'impressive', 'welcoming', 'professional', 'beautiful', 'exciting', 'positive', 'solid', 'amazing', 'wonderful', 'lovely']\n",
    "neg_adj = ['bad', 'boring', 'unpleasant', 'difficult', 'uncomfortable', 'ugly', 'poor', 'disappointing', 'sad', 'annoying', 'dirty', 'frustrating', 'depressing', 'nasty', 'horrible', 'stupid', 'negative', 'awful', 'stressful', 'irritating', 'disgusting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_eg = editor.template('{sub}{really} {p} the {n}.', sub=subj, really=[' really', ''], n=nouns, p=pos)\n",
    "pos_eg += editor.template('The {n} was{very} {p}.', sub=subj, very=[' very', ''], n=nouns, p=pos_adj)\n",
    "neg_eg = editor.template('{sub}{really} {p} the {n}.', sub=subj, really=[' really', ''], n=nouns, p=neg)\n",
    "neg_eg += editor.template('The {n} was{very} {p}.', sub=subj, very=[' very', ''], n=nouns, p=neg_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 1080)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_eg), len(neg_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = Mft(pos_eg, labels=1)\n",
    "test_neg = Mft(neg_eg, labels=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pos.set_expect(Expect.single(Expect.eq()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1494 examples\n",
      "Predicting 1080 examples\n"
     ]
    }
   ],
   "source": [
    "test_pos.run(pp)\n",
    "test_neg.run(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1080\n",
      "Failure rate:   1.7%\n",
      "\n",
      "Example fails:\n",
      "1 (1.0) The cabin crew was very stressful.\n",
      "1 (1.0) The flight was very stressful.\n",
      "1 (1.0) The flight was stressful.\n",
      "1 (1.0) The aircraft was stressful.\n",
      "1 (1.0) The seat was very stressful.\n"
     ]
    }
   ],
   "source": [
    "test_neg.summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1494\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "test_pos.summary(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = [x[0][0] for x in tg.unmask('John is a black <mask>.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = ['black', 'white', 'hispanic', 'asian']\n",
    "d = editor.template(['{male} is a %s {adj}.' % r for r in races], adj=adj, return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 8000 examples\n"
     ]
    }
   ],
   "source": [
    "test = Inv(d, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     2000\n",
      "Failure rate:   95.0%\n",
      "\n",
      "Example fails:\n",
      "0 (0.1) Gabriel is a black Muslim.\n",
      "1 (0.9) Gabriel is a hispanic Muslim.\n",
      "1 (0.9) Gabriel is a asian Muslim.\n",
      "\n",
      "0 (0.0) James is a black person.\n",
      "1 (0.9) James is a hispanic person.\n",
      "1 (0.9) James is a asian person.\n",
      "\n",
      "0 (0.1) Jason is a black girl.\n",
      "1 (1.0) Jason is a hispanic girl.\n",
      "1 (1.0) Jason is a asian girl.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12 black\n",
      "0.19 white\n",
      "0.96 hispanic\n",
      "0.98 asian\n"
     ]
    }
   ],
   "source": [
    "pps = np.array([[x[i][1] for i in range(4)] for x in test.results.confs])\n",
    "for i in range(4):\n",
    "    print('%.2f %s' % (pps[:, i].mean(), races[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, m = editor.template('{male} is a {r} {adj}.', r=races,  adj=adj, return_maps=True, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 'black', 'adj': 'man', 'male': 'Michael'}"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None], dtype=object)"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def is_fair(data, preds, confs, labels, metas):\n",
    "    rets = collections.defaultdict(lambda: [])\n",
    "    ret = np.repeat(None, len(data))\n",
    "    for c, m in zip(confs, metas):\n",
    "        rets[m['r']].append(c[1])\n",
    "    for i, r in enumerate(rets):\n",
    "        rets[r] = np.mean(rets[r])\n",
    "        ret[i] = rets[r]\n",
    "    print(rets)\n",
    "    return ret \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n",
      "defaultdict(<function is_fair.<locals>.<lambda> at 0x7f126044f598>, {'white': 0.356781789360361, 'asian': 0.9733978605270386, 'hispanic': 0.9593281026544243, 'black': 0.12038368929643184})\n"
     ]
    }
   ],
   "source": [
    "ex = Expect.test(is_fair)\n",
    "test = Mft(d, expect=ex, meta=m)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bad idea, fairness tests should just be INVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inv, dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = csv.DictReader(open('/home/marcotcr/datasets/airline/Tweets.csv'))\n",
    "labels = []\n",
    "confs = []\n",
    "airlines = []\n",
    "tdata = []\n",
    "reasons = []\n",
    "for row in r:\n",
    "    sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "    labels.append(sentiment)\n",
    "    confs.append(conf)\n",
    "    airlines.append(airline)\n",
    "    tdata.append(text)\n",
    "    reasons.append(row['negativereason'])\n",
    "\n",
    "mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "labels = np.array([mapping[x] for x in labels]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tdata\n",
    "parsed_data = list(nlp.pipe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_typo(string):\n",
    "    string = list(string)\n",
    "    swaps = 1\n",
    "    swaps = np.random.choice(len(string) - 1, swaps)\n",
    "    for swap in swaps:\n",
    "        swap = np.random.choice(len(string) - 1)\n",
    "        tmp = string[swap]\n",
    "        string[swap] = string[swap + 1]\n",
    "        string[swap + 1] = tmp\n",
    "    return ''.join(string)\n",
    "\n",
    "def add_typos(string):\n",
    "    return list(set([add_typo(string) for _ in range(10)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 100), add_typos, keep_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1043 examples\n"
     ]
    }
   ],
   "source": [
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     100\n",
      "Failure rate:   24.0%\n",
      "\n",
      "Example fails:\n",
      "0.0 @JetBlue Not for the dates or destination I'm headed \n",
      "0.5 @JetBlu eNot for the dates or destination I'm headed \n",
      "\n",
      "0.5 @AmericanAir will do. I also passed the website around to other passengers.\n",
      "0.7 @AmericanAir will do. I also passed the website around to other psasengers.\n",
      "0.7 @AmericanAir will do. I also passed the ewbsite around to other passengers.\n",
      "\n",
      "1.0 @united DM'ed you\n",
      "0.1 @uinted DM'ed you\n",
      "0.1 @united DM'ed yuo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2000 examples\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_typo, keep_original=True)\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     1000\n",
      "Failure rate:   4.9%\n",
      "\n",
      "Example fails:\n",
      "0.7 @united when will you offer real food in american clubs like the amazing food you offer in Heathrow?\n",
      "0.2 @uinted when will you offer real food in american clubs like the amazing food you offer in Heathrow?\n",
      "\n",
      "1.0 @united maybe on my return trip \n",
      "0.1 @united maybe on my erturn trip \n",
      "\n",
      "0.7 @united was in the air. Just DMd you\n",
      "0.0 @united was in the ai.r Just DMd you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get from somethwere else\n",
    "def add_negatives(string):\n",
    "    string = string.strip('.')\n",
    "    return [string + '. ' + l for l in ['I hate you', 'I despise you', 'You suck']]\n",
    "def add_positive(string):\n",
    "    string = string.strip('.')\n",
    "    return [string + '. ' + l for l in ['I love you', 'I like you', 'You are great']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4000 examples\n",
      "Test cases:     1000\n",
      "Filtered cases: 259 (25.9%)\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 1000), add_negatives, keep_original=True)\n",
    "expect_fn = Expect.monotonic(1, increasing=False, tolerance=0.1)\n",
    "test = Dir(data, expect_fn)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=False)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:     100\n",
      "Filtered cases: 72 (72.0%)\n",
      "Failure rate:   0.0%\n"
     ]
    }
   ],
   "source": [
    "data = Perturb.perturb(np.random.choice(sentences, 100), add_positive, keep_original=True)\n",
    "expect_fn = Expect.monotonic(1, increasing=True, tolerance=0.1)\n",
    "test = Dir(data, expect_fn)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=True)\n",
    "test.summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp = model_wrapper.ModelWrapper()\n",
    "pp = PredictorWrapper.wrap_softmax(qqp.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\tWhat does the Quran say about homosexuality?\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qs = []\n",
    "labels = []\n",
    "all_questions = set()\n",
    "for x in open('/home/marcotcr/datasets/glue/glue_data/QQP/dev.tsv').readlines()[1:]:\n",
    "    try:\n",
    "        q1, q2, label = x.strip().split('\\t')[3:]\n",
    "    except:\n",
    "        print(x)\n",
    "        continue\n",
    "    all_questions.add(q1)\n",
    "    all_questions.add(q2)\n",
    "    qs.append((q1, q2))\n",
    "    labels.append(label)\n",
    "labels = np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran = [x[0][0] for x in tg.unmask('What does the Quran say about <mask>?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran.remove('homosexuals')\n",
    "quran.remove('gays')\n",
    "quran.remove('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 272 examples\n",
      "Test cases:     272\n",
      "Failure rate:   2.2%\n",
      "\n",
      "Example fails:\n",
      "0.6 ('What does the Quran say about polygamy?', 'What does the Quran say about circumcision?')\n",
      "0.7 ('What does the Quran say about circumcision?', 'What does the Quran say about polygamy?')\n",
      "0.8 ('What does the Quran say about Muhammad?', 'What does the Quran say about Muslims?')\n",
      "1.0 ('What does the Quran say about Islam?', 'What does the Quran say about Muslims?')\n",
      "0.8 ('What does the Quran say about Muslims?', 'What does the Quran say about Islam?')\n"
     ]
    }
   ],
   "source": [
    "data = editor.template(('What does the Quran say about {thing1}?', 'What does the Quran say about {thing2}?'),\n",
    "                thing1=quran, thing2=quran, remove_duplicates=True)\n",
    "test = Mft(data, labels=0)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bible', 'book', 'report', 'survey', 'poll', 'study', 'Constitution', 'data', 'bible', 'election', 'census', 'law', 'UN', 'Quran', 'evidence', 'science', 'constitution', 'Koran', 'vote', 'bill', 'film', 'Holocaust', 'research', 'Pope', 'letter', 'article', 'verdict', 'world', 'movie', 'government', 'video', 'Torah', 'song', 'moon', 'public', 'dictionary', 'president', 'referendum', 'ACA', 'eclipse', 'church', 'US', 'media', 'Church', 'pope', 'record', 'test', 'answer', 'literature', 'Prophet', 'CDC', 'FDA', 'WHO', 'scripture', 'community', 'Buddha', 'ban', 'prophet', 'statement', 'interview', 'Gospel', 'Book', 'past', 'question', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print([x[0][0] for x in tg.unmask_multiple(editor.template('What does the <mask> say about {thing}?', thing=quran))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1530 examples\n",
      "Test cases:     1530\n",
      "Failure rate:   8.9%\n",
      "\n",
      "Example fails:\n",
      "0.6 ('What does the Bible say about homosexuality?', 'What does the Church say about homosexuality?')\n",
      "1.0 ('What does the Gospel say about you?', 'What does the Bible say about you?')\n",
      "0.9 ('What does the Quran say about religion?', 'What does the Prophet say about religion?')\n",
      "0.6 ('What does the Bible say about women?', 'What does the Torah say about women?')\n",
      "1.0 ('What does the Bible say about God?', 'What does the Gospel say about God?')\n"
     ]
    }
   ],
   "source": [
    "books = ['Bible', 'Constitution', 'Quran', 'Pope', 'Torah', 'Church', 'Buddha', 'Prophet', 'Gospel', 'Book']\n",
    "data = editor.template(('What does the {book1} say about {thing}?', 'What does the {book2} say about {thing}?'),\n",
    "                thing=quran, book1=books, book2=books, remove_duplicates=True)\n",
    "test = Mft(data, labels=0)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inv, dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_map =  pickle.load(open('/home/marcotcr/tmp/processed_qqp.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_qs = [spacy_map[x[0]] for x in qs] + [spacy_map[x[1]] for x in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pairs = [(spacy_map[x[0]],spacy_map[x[1]]) for x in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(pair):\n",
    "    x0, x1 = pair\n",
    "    ents_0 = set([x[0].text for x in x0.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ents_1 = set([x[0].text for x in x1.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ret = []\n",
    "    if ents_0 and ents_0.intersection(ents_1):\n",
    "        for e in ents_0.intersection(ents_1):\n",
    "            if e == 'Quora':\n",
    "                continue\n",
    "            ret.extend([(pair[0].text.replace(e, n), pair[1].text.replace(e, n)) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        return [ret[i] for i in idxs]\n",
    "# change_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9792 examples\n",
      "Test cases:     1632\n",
      "Failure rate:   11.1%\n",
      "\n",
      "Example fails:\n",
      "0.2 (\"What is India's relationship with Bangladesh?\", \"What is Bangladesh's relationship with India?\")\n",
      "0.9 (\"What is India's relationship with Isabella?\", \"What is Isabella's relationship with India?\")\n",
      "0.9 (\"What is India's relationship with Jesus?\", \"What is Jesus's relationship with India?\")\n",
      "\n",
      "1.0 ('Which is best place to stay and visit in Kerala?', 'What are the best 10 places to visit in Kerala including any falls?')\n",
      "0.4 ('Which is best place to stay and visit in Sara?', 'What are the best 10 places to visit in Sara including any falls?')\n",
      "0.4 ('Which is best place to stay and visit in Emily?', 'What are the best 10 places to visit in Emily including any falls?')\n",
      "\n",
      "1.0 ('What is Jake Williamss history that made him into a narcissist?', 'How is Jake Williams a narcissist?')\n",
      "0.3 ('What is Sara Williamss history that made him into a narcissist?', 'How is Sara Williams a narcissist?')\n",
      "0.4 ('What is Nicole Williamss history that made him into a narcissist?', 'How is Nicole Williams a narcissist?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = np.random.choice(len(processed_pairs), 2000,  replace=False)\n",
    "# idxs = np.random.choice(len(processed_pairs), len(processed_pairs),  replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_names, keep_original=True)\n",
    "# test = Dir(data, Expect.wrap(Expect.all(Expect.pairwise_to_group(Expect.monotonic_label(1, increasing=True, tolerance=0)))) )\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 432 examples\n",
      "Test cases:     72\n",
      "Failure rate:   5.6%\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What are the best places to visit in Wayanad, Kerala?', 'What can be the medium budget to visit best places in Kerala for three members (2-3 days)?')\n",
      "0.2 ('What are the best places to visit in Wayanad, Isaiah?', 'What can be the medium budget to visit best places in Isaiah for three members (2-3 days)?')\n",
      "0.0 ('What are the best places to visit in Wayanad, Jamie?', 'What can be the medium budget to visit best places in Jamie for three members (2-3 days)?')\n",
      "\n",
      "0.9 ('Was Donald Trump always rich?', 'How rich is Donald Trump?')\n",
      "0.5 ('Was Jason Trump always rich?', 'How rich is Jason Trump?')\n",
      "\n",
      "1.0 ('What is the best way to start contributing to the Linux kernel?', 'How should I start contributing for Linux?')\n",
      "0.2 ('What is the best way to start contributing to the Gabriel kernel?', 'How should I start contributing for Gabriel?')\n",
      "0.0 ('What is the best way to start contributing to the Elizabeth kernel?', 'How should I start contributing for Elizabeth?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idxs = np.random.choice(len(processed_pairs), 2000,  replace=False)\n",
    "# idxs = np.random.choice(len(processed_pairs), len(processed_pairs),  replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_names, keep_original=True)\n",
    "# test = Dir(data, Expect.wrap(Expect.all(Expect.pairwise_to_group(Expect.monotonic_label(1, increasing=True, tolerance=0)))) )\n",
    "test = Inv(data, threshold=0.1)\n",
    "test.run(pp, overwrite=True)\n",
    "test.summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.results.passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name_in_one(pair):\n",
    "    x0, x1 = pair\n",
    "    ents_0 = set([x[0].text for x in x0.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ents_1 = set([x[0].text for x in x1.ents if x[0].ent_type_ == 'PERSON'])\n",
    "    ret = []\n",
    "    if ents_0 and ents_0.intersection(ents_1):\n",
    "        for e in ents_0.intersection(ents_1):\n",
    "            if e == 'Quora':\n",
    "                continue\n",
    "            ret.extend([(pair[0].text.replace(e, n), pair[1].text) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "            ret.extend([(pair[0].text, pair[1].text.replace(e, n)) for n in editor.lexicons['male'] + editor.lexicons['female']])\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        return [ret[i] for i in idxs]\n",
    "# change_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 756 examples\n",
      "Test cases:     126\n",
      "Filtered cases: 64 (50.8%)\n",
      "Failure rate:   3.2%\n",
      "\n",
      "Example fails:\n",
      "0.1 ('Why does Donald Trump think the debate schedule favors Hillary?', 'Why is Donald Trump saying the debate schedule is unfair? Is it because his support base would rather watch the NFL than his debate?')\n",
      "0.2 ('Why does Donald Trump think the debate schedule favors Hillary?', 'Why is Ava Trump saying the debate schedule is unfair? Is it because his support base would rather watch the NFL than his debate?')\n",
      "\n",
      "0.5 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Donald Trump winning the Republican nomination?')\n",
      "1.0 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Adam Trump winning the Republican nomination?')\n",
      "1.0 ('What do you feel about Donald Trump winning the elections?', 'How do you feel about Austin Trump winning the Republican nomination?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monotonic = Expect.monotonic(1, increasing=False, tolerance=0.1)\n",
    "idxs = np.random.choice(len(processed_pairs), 3000, replace=False)\n",
    "sl = [processed_pairs[i] for i in idxs]\n",
    "data = Perturb.perturb(sl, change_name_in_one, keep_original=True)\n",
    "test = Dir(data, monotonic)\n",
    "test.run(pp, overwrite=True)\n",
    "test.set_monotonic_print(label=1, increasing=False)\n",
    "test.summary(n=3)\n",
    "# test.results.passed[test.results.passed != None].astype(bool).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only consider examples that were duplicates\n",
    "sl_fn = lambda pred, *args, **kwargs: pred == 1\n",
    "# This substitutes the previous one\n",
    "# def sl_fn(x, pred, *args, **kwargs):\n",
    "#     print(pred)\n",
    "#     return pred[0] == 1\n",
    "sl_fn2 = lambda x, pred, *args, **kwargs: pred[0] == 1\n",
    "is_false = Expect.single(Expect.eq(0), agg_fn=Expect.all(ignore_first=True))#, slice_fn=sl_fn)\n",
    "# is_false = Expect.slice_pairwise(is_false, sl_fn)\n",
    "is_false = Expect.slice_testcase(is_false, sl_fn2)\n",
    "test.set_expect(is_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     126\n",
      "Filtered cases: 53 (42.1%)\n",
      "Failure rate:   52.1%\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What are the best books on Joseph Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "1.0 ('What are the best books on Peter Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "1.0 ('What are the best books on Jesse Goebbels?', 'Are there any really, really interesting books on Joseph Goebbels?')\n",
      "\n",
      "1.0 ('How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sahara?', 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sonoran Desert?')\n",
      "1.0 ('How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sahara?', 'How cold can Tiffany Gobi Desert get, and how do its average temperatures compare to Tiffany ones in Tiffany Sonoran Desert?')\n",
      "1.0 ('How cold can Ella Gobi Desert get, and how do its average temperatures compare to Ella ones in Ella Sahara?', 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Sonoran Desert?')\n",
      "\n",
      "0.9 ('Should I read A Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "0.9 ('Should I read Ian Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "0.9 ('Should I read Christina Song of Ice and Fire or watch Game of Thrones first?', 'Should I read A Song of Ice and Fire after watching the Game of Thrones TV series?')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from checklist.inv_dir import pairwise_print_fn\n",
    "def fail_cr(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != 0\n",
    "def sort_cr(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return orig_conf[0]\n",
    "print_fn = pairwise_print_fn(fail_cr, sort_cr)\n",
    "test.summary(n=3, print_fn=print_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltests import bert_squad_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_squad_model.BertSquad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just makes confidence=1 for every prediction\n",
    "pp = PredictorWrapper.wrap_predict(model.predict_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2415e533d24776ac9f9d70c9e2c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mary']"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_pairs([('Who is dumb?', 'Mary is somewhat dumb. John is not so dumb.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f75826c94941a5a642cf712e25dfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Mary'], array([1.]))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp([('Who is dumb?', 'Mary is somewhat dumb. John is not so dumb.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drinking', 'smoking', 'crying', 'reading', 'writing', 'worrying', 'eating', 'driving', 'listening', 'praying', 'talking', 'working', 'trying', 'calling', 'laughing']\n"
     ]
    }
   ],
   "source": [
    "verbs = [x[0][0] for x in tg.unmask('Luke told Mary that she should probably stop <mask>.')]\n",
    "verbs = [x for x in verbs if 'ing' in x]\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data, maps = editor.template(('Who is currently {verb}?', '{male} told {female} she should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['female'] for m in maps]\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{male} told {female} he should probably stop {verb}'), nsamples=100,verb=verbs, return_maps=True)\n",
    "labels += [m['male'] for m in maps]\n",
    "data += n\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{female} told {male} he should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['male'] for m in maps]\n",
    "data += n\n",
    "n, maps = editor.template(('Who is currently {verb}?', '{female} told {male} she should probably stop {verb}'), nsamples=100, verb=verbs, return_maps=True)\n",
    "labels += [m['female'] for m in maps]\n",
    "data += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Who is currently worrying?',\n",
       " 'Juan told Crystal she should probably stop worrying')"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f8fa8a43d1438cbc766449fd7abdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = Mft(data, labels=labels)\n",
    "test.run(pp)\n",
    "# test.results.passed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):\n",
    "    q, c = x\n",
    "    ret = 'C: %s\\nQ: %s\\n' % (c, q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_squad(x, pred, conf, label=None, *args, **kwargs):\n",
    "    q, c = x\n",
    "    ret = 'Q: %s\\n' % (q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:     400\n",
      "Failure rate:   58.2%\n",
      "\n",
      "Example fails:\n",
      "C: Taylor told Carlos she should probably stop praying\n",
      "Q: Who is currently praying?\n",
      "A: Taylor\n",
      "P: Carlos\n",
      "\n",
      "C: Carlos told Rachel he should probably stop worrying\n",
      "Q: Who is currently worrying?\n",
      "A: Carlos\n",
      "P: Rachel\n",
      "\n",
      "C: Peter told Chloe she should probably stop trying\n",
      "Q: Who is currently trying?\n",
      "A: Chloe\n",
      "P: Peter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.summary(n=3, format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked = [x[0][0] for x in tg.unmask('John was told by Luke that he really likes <mask>.', beam_size=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data, maps = editor.template(('Who really likes {liked}?', '{male1} was told by {male2} that {male2} really likes {liked}.'), male1=editor.lexicons['male'], male2=editor.lexicons['male'], nsamples=100, verb=verbs, liked=liked, return_maps=True, remove_duplicates=True)\n",
    "labels += [m['male2'] for m in maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a2d7705454f35b5de65d29223020a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test cases:     100\n",
      "Failure rate:   44.0%\n",
      "\n",
      "Example fails:\n",
      "C: Eric was told by Austin that Austin really likes Hunter.\n",
      "Q: Who really likes Hunter?\n",
      "A: Austin\n",
      "P: Eric\n",
      "\n",
      "C: Ethan was told by Juan that Juan really likes Jess.\n",
      "Q: Who really likes Jess?\n",
      "A: Juan\n",
      "P: Juan that Juan\n",
      "\n",
      "C: Robert was told by Ian that Ian really likes Vanessa.\n",
      "Q: Who really likes Vanessa?\n",
      "A: Ian\n",
      "P: Robert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = Mft(data, labels=labels)\n",
    "test.run(pp)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bible', 'book', 'report', 'survey', 'poll', 'study', 'Constitution', 'data', 'bible', 'election', 'census', 'law', 'UN', 'Quran', 'evidence', 'science', 'constitution', 'Koran', 'vote', 'bill', 'film', 'Holocaust', 'research', 'Pope', 'letter', 'article', 'verdict', 'world', 'movie', 'government', 'video', 'Torah', 'song', 'moon', 'public', 'dictionary', 'president', 'referendum', 'ACA', 'eclipse', 'church', 'US', 'media', 'Church', 'pope', 'record', 'test', 'answer', 'literature', 'Prophet', 'CDC', 'FDA', 'WHO', 'scripture', 'community', 'Buddha', 'ban', 'prophet', 'statement', 'interview', 'Gospel', 'Book', 'past', 'question', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print([x[0][0] for x in tg.unmask_multiple(editor.template('What does the <mask> say about {thing}?', thing=quran))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_squad(fold='validation'):\n",
    "    answers = []\n",
    "    data = []\n",
    "    ids = []\n",
    "    files = {\n",
    "        'validation': '/home/marcotcr/datasets/squad/dev-v1.1.json',\n",
    "        'train': '/home/marcotcr//datasets/squad/train-v1.1.json',\n",
    "        }\n",
    "    f = json.load(open(files[fold]))\n",
    "    for t in f['data']:\n",
    "        for p in t['paragraphs']:\n",
    "            context = p['context']\n",
    "            for qa in p['qas']:\n",
    "                data.append({'passage': context, 'question': qa['question'], 'id': qa['id']})\n",
    "                answers.append(set([(x['text'], x['answer_start']) for x in qa['answers']]))\n",
    "    return data, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, answers =  load_squad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(x['question'], x['passage']) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(x):\n",
    "    q, c = x\n",
    "    in_p = set()\n",
    "    not_in_p = set()\n",
    "    for n in editor.lexicons['male'][:10]:\n",
    "        if re.search(r'\\b%s\\b' % n, c):\n",
    "            in_p.add(n)\n",
    "        else:\n",
    "            not_in_p.add(n)\n",
    "    if not in_p:\n",
    "        return None\n",
    "    ret = []\n",
    "    ret_add = []\n",
    "    for p in in_p:\n",
    "        for n in not_in_p:\n",
    "            ret.append((re.sub(r'\\b%s\\b' % p, n, q), re.sub(r'\\b%s\\b' % p, n, c)))\n",
    "            ret_add.append((p, n))\n",
    "    if ret:\n",
    "        idxs = np.random.choice(len(ret), min(5, len(ret)), replace=False)\n",
    "        ret = [ret[i] for i in idxs]\n",
    "        ret_add = [ret_add[i] for i in idxs]\n",
    "    return ret, ret_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i, x in enumerate(pairs) if 'John' in x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2058 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37654cb9177d49eca96387bc6ffd2792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=268.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test cases:     343\n",
      "Failure rate:   1.2%\n",
      "\n",
      "Example fails:\n",
      "Q: What did John Dobson describe Newcastle as?\n",
      "P: neoclassical centre referred to as Tyneside Classical\n",
      "\n",
      "John -> Daniel\n",
      "Q: What did Daniel Dobson describe Newcastle as?\n",
      "P: neoclassical\n",
      "\n",
      "\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor\n",
      "\n",
      "John -> James\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor meets an unknown incarnation of himself\n",
      "\n",
      "John -> Joseph\n",
      "Q: What Doctor was first referred to as \"his secret\"?\n",
      "P: Eleventh Doctor meets an unknown incarnation of himself\n",
      "\n",
      "\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: both Kenia and Kegnia\n",
      "\n",
      "Joseph -> James\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: Kenia and Kegnia\n",
      "\n",
      "Joseph -> William\n",
      "Q: Ludwig Krapf recorded the name was what?\n",
      "P: Kenia and Kegnia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def new_eq(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    if meta:\n",
    "        p, n = meta\n",
    "        orig_pred =  re.sub(r'\\b%s\\b' % p, n, orig_pred)\n",
    "    return pred == orig_pred\n",
    "\n",
    "def format_name(x, pred, conf, label=None, meta=None):\n",
    "    ret = ''\n",
    "    if meta is not None and len(meta):\n",
    "        ret = '%s -> %s\\n' % meta\n",
    "    ret += format_squad(x, pred, conf, label, meta)\n",
    "    return ret\n",
    "    \n",
    "# tt = Expect.wrap(Expect.all(Expect.pairwise_to_group(new_eq), ignore_first=True))\n",
    "random_idxs = np.random.choice(len(pairs), 3000)\n",
    "data, meta = Perturb.perturb([pairs[i] for i in random_idxs], change_name, returns_additional=True)\n",
    "tt = Expect.pairwise(new_eq)\n",
    "test = Inv(data, expect=tt, meta=meta)\n",
    "test.run(pp)\n",
    "test.summary(n=3, format_example_fn=format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " ('John', 'Joshua'),\n",
       " ('John', 'Matthew'),\n",
       " ('John', 'William'),\n",
       " ('John', 'Christopher'),\n",
       " ('John', 'Michael'),\n",
       " ('John', 'James'),\n",
       " ('John', 'David'),\n",
       " ('John', 'Daniel'),\n",
       " ('John', 'Joseph')]"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['John Fox', 'Joshua Fox', 'Matthew Fox', 'William Fox',\n",
       "       'Christopher Fox', 'Michael Fox', 'James Fox', 'David Fox',\n",
       "       'Daniel Fox', 'Joseph Fox'], dtype='<U15')"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.results.preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function checklist.expect.Expect.all.<locals>.expect(xs, preds, confs, labels=None, meta=None)>"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John', 'Joshua')"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_name(pairs[178])[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in pairs if 'John' in x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "How is vanilla extract made?"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_qs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Luke', 'Mark']"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons['male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40430"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NORP'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_qs[0].ents[0][0].ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor.template('This is {bad}',  bad=['bad', 'great', 'awesome'], return_maps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('This is bad', 'This is not bad'),\n",
       "  ('This is great', 'This is not great'),\n",
       "  ('This is awesome', 'This is not awesome')],\n",
       " [{'bad': 'bad'}, {'bad': 'great'}, {'bad': 'awesome'}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "editor.template(('This is {bad}', 'This is not {bad}'),  bad=['bad', 'great', 'awesome'], return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bad': 'This is bad', 'notbad': 'This is not bad'},\n",
       " {'bad': 'This is great', 'notbad': 'This is not great'},\n",
       " {'bad': 'This is awesome', 'notbad': 'This is not awesome'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template({\n",
    "    'bad': 'This is {bad}',\n",
    "    'notbad': 'This is not {bad}'},  bad=['bad', 'great', 'awesome'], return_maps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'bad': 'This is {bad}',\n",
    "    'notbad': ('this is not {bad}', 'this is quite {abad}')}\n",
    "b = ({'bad': 'This is {bad}', 'notbad': 'This is not {bad}'}, 'This is quite {bad}')\n",
    "c = 'This is quite {bad}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': ['John', 'Luke', 'Mark'], 'female': ['Mary', 'Judy', 'Julia']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is terrible, John',\n",
       " 'This is bad, John',\n",
       " 'This is terrible, Luke',\n",
       " 'This is bad, Luke',\n",
       " 'This is terrible, Mark',\n",
       " 'This is bad, Mark']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.template('This is {bad}, {male}', bad=('terrible', 'bad'), nsamples=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
